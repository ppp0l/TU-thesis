\section{Introduction} \label{sec:intro}


\subsection{Motivation}\label{sec:motivation}

When dealing with real-world phenomena, adopting a mathematical representation is often necessary in order to simplify complexity, obtain insight about underlying features, and produce sufficiently accurate predictions on the behaviour of the considered portion of reality. 
In the present day, computer simulations and scientific computing have become essential parts of mathematical modelling, rendering it possible to adopt numerical techniques to solve analytically untractable problems.
Moreover, as numerous and vast families of mathematical models are available to adequately represent different classes of processes, the modeller is presented with choices about model complexity and desired characteristics, and with tasks such as parameter identification, model order reduction and uncertainty quantification. 
This work involves all three of these tasks in different manners and measures. 

The problem of the calibration of parameters for a computationally expensive numerical model is considered: a bayesian perspective is adopted in formulating the inverse problem, and then the cost of model evaluation is reduced by considering a regression-based surrogate model, instead of classical model order reduction techniques such as Proper Orthogonal Decomposition or Reduced-Basis methods.
The considered regression techniques are Gaussian Process Regression, which is commonly employed in Bayesian Inverse Problems as it provides not only a pointwise estimate but also a quantification of its confidence, and Lipschitz Regression, which is a less utilized technique that also provides a stochatic prediction but relies on different assumptions than Gaussian Process Regression. 
The motivation for this work lies precisely in the assumptions required by the two techniques.

We consider a model 
\[
 y
\]



\subsection{Literature and related works}\label{sec:literature}
