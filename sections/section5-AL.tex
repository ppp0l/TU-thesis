\section{Active learning}\label{sec:AL}
The last section established the surrogate-based likelihoods $L_{D, \text{GPR}}$ and $L_{D, \text{LR}}$, given by Equation \eqref{eq:GPR-likelihood} and Equation~\eqref{eq:LR-likelihood}, respectively.
These two likelihoods are then used to compute the surrogate-informed posteriors $\pi_{P\mid D, Y^m = y^m} $ given by Equation~\eqref{eq:surr-posterior}.

From a practical perspective, this is the only accessible posterior: it is consequently crucial to make sure that it approximates the true posterior $\pi_{P\mid Y^m = y^m}$ as closely as possible.
The quality of the approximation of the posterior is directly related to the quality of the surrogate model, which in turn depends on the training data $D$.
Consequently, we now turn our attention to the problem of selecting the training data in an optimal way, in order to maximize the quality of the posterior approximation under computational budget constraints.

We adopt an Active Learning (AL) framework, aiming to select the most informative points to evaluate the forward model $y$; the forward model evaluations are inexact but thanks to the FE adaptivity the tolerance on the discretization error can be controlled, so we also aim at selecting the optimal tolerance level for each evaluation.
The adaptive selection of the tolerance level has been shown to be particularly effective in reducing the computational costs for GPR in an IP setting, both for MAP estimates~\cite{SemlerWeiser2023,SemlerWeiser2024} and for posterior sampling~\cite{VillaniUngerWeiser2024}.
The methodology of the latter works is expanded and adapted to the present setting, considering the LR surrogate model and different target functions, and incorporating a discretization error covariance estimate for GPR. 

\subsection{General methodology}\label{sec:AL-method}

In order to describe the strategy, we will need to introduce some new elements and notation. \medbreak 

We start by considering a local error indicator function 
\begin{equation} \label{eq:loc-err-ind}
    e_{D} : \Omega \to \mathbb R
\end{equation} 
which quantifies the uncertainty at a given point $p \in \Omega$ of the surrogate model $y_D$ with training data $D=\{(p_i, \tau_i, y_i)\}_{i=1}^n$.
The expected error estimate for data $D$ is then given by
\begin{equation} \label{eq:glob-err-ind}
    E(D) = \int_{\Omega} e_{D}(p) \, \pi_{ Y^m = y^m}(p) \, dp.
\end{equation}

Moreover, we consider to have a certain computational budget $W$ to spend on the evaluations of the forward model $y$ necessary to generate the training data $D$. \medbreak 

In a design of experiments (DoE) setting, the problem of selecting training data $D$ can be formulated to selecting a design $\mc D = \{ (p_i, \tau_i) \}_{i=1}^n$ of evaluation points and tolerances, whose evaluation through the numerical forward model then provides the training data $D(\mc D) = \{ (p_i, \tau_i, H(f_{\tau_i}(\cdot, p_i)) \}_{i=1}^n$. \newline
As the computational costs of the forward model evaluations depend on the tolerance level and not on the resulting value, we write $W(\mc D)$ for the computational cost for the creation of $D(\mc D)$. \newline
Finally, it useful to talk about refinements of a design: for two designs $\mc D$ and $\mc D'$, we say that $\mc D'$ refines $\mc D$, denoted by $\mc D' \leq \mc D$, if for every $(p, \tau) \in \mc D$ there exists $(p, \tau') \in \mc D'$ such that $\tau' \leq \tau$, i.e.  $\mc D'$ contains all the evaluation points of $\mc D$ with a smaller or equal tolerance.
For $\mc D' \leq \mc D$, we write 
\[ 
W(\mc D' \mid D(\mc D)) \coloneq W(\mc D') - W(\mc D)
\] for the computational cost of computing $D(\mc D')$ having already computed $D(\mc D)$. \medbreak

By using the expected error $E(D)$ as an evaluation metric and the computational budget $W$ as a constraint, we can formulate the following optimal DoE problem:
\begin{equation} \label{prob:doe}
    \min_{\mc D} E(D(\mc D)) \quad \text{subject to} \quad W(\mc D) \le W,
\end{equation}
This problem cannot be solved directly, as the evaluation of $E(D(\mc D))$ requires knowledge of training data $D$ which is not available before the design is fixed and the forward model evaluations are performed.
A variety of approaches to optimal DoE problems exist~\cite{HuanJagalurMarzouk2024} and in the present setting we resort to a greedy sequential approach. 
This approach is based on the idea of incrementally selecting new points and tolerances to update the design by considering the information gathered so far, evaluating the model each step. \newline
This is a compromise between static designs, which first select the points and then evaluate the model, and Bayesian sequential lookahead methods, which marginalize on the outcome of future experiments at each step; while the latter can be extremely effective, in our setting the design includes not only the evaluation points but also the tolerance levels, rendering the greedy approach the most suitable. \medbreak

The sequential approach considers a fractionating of the budget $\Delta W_1, \dots, \Delta W_J$ and produces a sequence of designs $\mc D_0,\dots, \mc D_J$, where $\mc D_j$ refines $\mc D_{j-1}$ for every $j \in \{1,\dots, J\}$.
The designs are obtained by solving the following sequence of DoE problems:
\begin{equation} \label{prob:incremental-doe}
    \min_{\mc D_{j} \le \mc D_{j-1}} E(D(\mc D_{j})) \quad \text{s.t.} 
    \quad W(\mc D_{j} \mid D(\mc D_{j-1}) ) \le \Delta W_j, \ \text{ for every } \ j=1,\dots,J.
\end{equation} 
Notice that at each step, Problem~\eqref{eq:incremental-doe} is not yet directly numerically solvable; in fact:
\begin{enumerate}[label=\textbf{\arabic*}]    
    \item $E(D(\mc D_{j}))$ involves the computation of an integral with respect to the unknown posterior $\pi_{Y^m = y^m}$;
    \item the problem is a mixed continuous-combinatorial optimization problem with possibly non convex objective and constraints;
    \item the computational cost associated with a design is not known before the simulations are performed.
\end{enumerate}
These issue are overcome separately in the following ways:
\begin{enumerate}[label=\textbf{\arabic*}]
    \item interleave sampling steps and training steps: sample from the approximated posterior $\pi_{D_{j-1}, Y^m = y^m}$ before step $j$, concatenating the new samples to the chain $\mc S_{j-1}$ and discarding old samples to obtain the current posterior representation $\mc S_j$;
    \item the choice of new evaluation points is separated from the optimization of evaluation accuracies, splitting the problem into two simpler subproblems;
    \item the asymptotical estimates described in Section~\ref{sec:AdaFE} are adopted to estimate the computational cost of a design, leading for $\mc D = \{ (p_i, \tau_i) \}_{i=1}^n$ to the estimate
    \begin{equation} \label{eq:comp-cost}
        W(\mc D) = \sum_{i=1}^n W_{\tau_i} = \sum_{i=1}^n \tau_i^{- \frac{l \cdot  s}{r}}.
    \end{equation}
\end{enumerate}

For point \textbf{2}, the selection of new candidate points at step $j$ is based on an acquisition function derived from the expected error $E(D)$
\begin{equation}\label{eq:acq-fun-gen}
    \mc A_j(p) = \int_{\Theta} \alpha_{\mc D_{j-1}}(p, p') \, \pi_{P \mid Y^m = y^m}(p') \, dp',
\end{equation}
where $\alpha_{\mc D_{j-1}}(p, p')$ quantifies the reduction of error in $p'$ when adding $p$ to the design $\mc D_{j-1}$. 
In the following sections, specific versions of $\alpha_{\mc D_{j-1}}$ for GPR and LR surrogates will be described. \newline
The optimization of the evaluation tolerances is performed by solving Problem~\ref{prob:incremental-doe} as a function of the accuracies.
Precisely, at step $j$ let $\mc D_{j-1} = \{ (p_i, \tau_i) \}_{i=1}^{n_{j-1}}$ and new candidates $\{ p_{n+i} \}_{i=1}^{s}$; the admissible tolerances for designs that refine $\mc D_{j-1}$ and add new points among the candidates are given by
\begin{equation*}
    \mc T_j =  \Big \{ (\tilde \tau _1, \dots, \tilde \tau _{n_{j-1}+s}) \in ( \R ^+ \cup \{ +\infty \} ) ^{n_{j-1}+s} \mid \tilde \tau_i  \leq \tau_ i \text{ for } i \leq n_{j-1}\Big \},
\end{equation*}
where for $\tau_i = +\infty$ the corresponding point $p_i$ is excluded from the design.
The tolerance problem at step $j$ is then given by
\begin{equation} \label{prob:tolerance-doe}
    \min_{\mc D_{j} \in \bb D_j} E(D(\mc D_{j})) \quad \text{s.t.} 
    \quad W(\mc D_{j} \mid D(\mc D_{j-1}) ) \le \Delta W_j, 
\end{equation} 
where \[
\bb D_j = \{ \mc D = \{ (p_i, \tau_i) \}_{i=1}^{n_{j-1}+s} \mid (\tau_1, \dots, \tau_{n_{j-1}+s}) \in \mc T_j \}.
\]
\medbreak

The following pseudo-algorithm summarizes the adopted methodology. \medskip

\par\noindent\rule[1mm]{\textwidth}{0.4pt}
\phantomsection \makeatletter\def\@currentlabel{Algorithm 1}\makeatother\label{algo:AGP}
\large{\textbf{Algorithm 1.} } \normalsize
\par\noindent\rule[2mm]{\textwidth}{0.2pt}
\textbf{Require:} initial design $\mc D_0$, fractionating of the budget $\Delta W_1,\dots, \Delta W_J$.
\par\noindent\rule[2mm]{\textwidth}{0.2pt}
Initialize sample chain $\mc S_0 = \emptyset$. \newline
Iterative solution of the training problem.
For $j =1, \dots, J$ do: 
\begin{enumerate}
    \item \textbf{Sample the posterior.} \newline
    Decide: $n_j$, $h_j \in \bb N$. \newline
    Sample $n_j$ points from $\pi_{P \mid D_{j-1}, Y^m = y^m}$. \newline
    Discard the $h_j$ oldest samples from $\mc S_{j-1}$. \newline
    Concatenate the new samples to the remaining old samples to obtain $\mc S_j$.

    \item \textbf{Select the candidates.} \newline 
    Decide: $s_j \in \bb N$. \newline
    For any $p\in \Theta$, approximate the acquisition function~\ref{eq:acq-fun-gen} by
    \begin{equation}\label{eq:acq-fun-disc}
        \bar{\mc A}_j(p) \coloneq \sum_{p' \in \mc S_j} \alpha_{\mc D_{j-1}}(p, p') .
    \end{equation}
    Select $s_j$  local maximizers of $\bar{\mc A}_j$.

    \item \textbf{Select the tolerances.} Solve the discretization 
    \begin{equation}\label{prob:discrete-tol}
    \end{equation} of Problem~\ref{prob:tolerance-doe} to determine the new evaluation tolerances $T = (\tau_1, \dots \tau_{s_j+r_j})$. If $\tau_i = \infty$ for some $i = s_j +1, \dots , s_j+r_j$, exclude the corresponding candidate $p_i$ from the design. This results in the new experimental design $\mc D_j  = \big ( (p_i, \tau_i))_{i = 1, \dots , s_{j+1}}$

    \item \textbf{Evaluate the model.} For every $(p, \tau )$ in $\mc D_j$ which was updated, i.e. the new evaluation points and the old points where the tolerance has changed, evaluate the model $y_\tau(p)$. Having obtained the new values $(y_1, \dots, y_{r_{j+1}})$, update the training set and re-train the surrogate to obtain $y_{\mc D_j}$.
\end{enumerate}
At last, the final set of samples $\mc S_{J+1}$ is obtained by drawing $n_{J+1}$ samples from $\pi(p \mid y^m, \mc D_J)$ and discarding the oldest $h_{J+1}$ samples, for some numbers  $n_{J+1},h_{J+1}$. 
\par\noindent\rule[3.5mm]{\textwidth}{0.4pt}
blablabla

\subsection{Active learning for GPR}\label{sec:GPAL}
\subsection{Active learning for LR}\label{sec:LRAL}
