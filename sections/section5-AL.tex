\section{Adaptive Design of Experiments}\label{sec:AL}
The last section established the surrogate-based likelihoods $L_{D, \text{GPR}}$ and $L_{D, \text{LR}}$, given by Equation \eqref{eq:GPR-likelihood} and Equation~\eqref{eq:LR-likelihood}, respectively.
These two likelihoods are then used to compute the surrogate-informed posteriors $\pi_{P\mid D, Y^m = y^m} $ given by Equation~\eqref{eq:surr-posterior}.

From a practical perspective, this is the only accessible posterior: it is consequently crucial to make sure that it approximates the true posterior $\pi_{P\mid Y^m = y^m}$ as closely as possible.
The quality of the approximation of the posterior is directly related to the quality of the surrogate model, which in turn depends on the training data $D$.
Consequently, we now turn our attention to the problem of selecting the training data in an optimal way, in order to maximize the quality of the posterior approximation under computational budget constraints.

We adopt a Design of Experiments (DoE) framework, aiming to select the most informative points to evaluate the forward model $y$; the forward model evaluations are inexact but thanks to the FE adaptivity the tolerance on the discretization error can be controlled, so we also aim at selecting the optimal tolerance level for each evaluation.
The adaptive selection of the tolerance level has been shown to be particularly effective in reducing the computational costs for GPR in an IP setting, both for MAP estimates~\cite{SemlerWeiser2023,SemlerWeiser2024} and for posterior sampling~\cite{VillaniUngerWeiser2024,VillaniArconesUngerWeiser2025}.
The methodology of the latter works is expanded and adapted to the present setting, considering the LR surrogate model and different target functions, and incorporating a discretization error covariance estimate for GPR. \medbreak

The remainder of this section is organized as follows: Section~\ref{sec:AL-method} introduces the notation and the general methodology for the adaptive DoE framework we consider, presenting the challenges of the adopted approach and the general structure of the algorithm; Section~\ref{sec:GPAL} describes the specific implementation of the DoE framework for GPR, introducing the GPR-specific error indicator function and acquisition function, and developing a strategy to estimate the noise covariance from the forward model evaluations; finally, Section~\ref{sec:LRAL} presents the implementation of the DoE framework for LR, introducing the LR-specific error indicator function, the acquisition function, and the necessary adjustments for the tolerance optimization problem for the LR surrogate model.

\subsection{General methodology}\label{sec:AL-method}

In order to describe the strategy, we will need to introduce some new elements and notation. \medbreak 

We start by considering a local error indicator function 
\begin{equation} \label{eq:loc-err-ind}
    e_{D} : \Omega \to \mathbb R
\end{equation} 
which quantifies the uncertainty at a given point $p \in \Omega$ of the surrogate model $y_D$ with training data $D=\{(p_i, \tau_i, y_i)\}_{i=1}^n$.
The expected error estimate for data $D$ is then given by
\begin{equation} \label{eq:glob-err-ind}
    E(D) = \int_{\Omega} e_{D}(p) \, \pi_{ Y^m = y^m}(p) \, dp.
\end{equation}

Moreover, we consider to have a certain computational budget $W$ to spend on the evaluations of the forward model $y$ necessary to generate the training data $D$. \medbreak 

In a DoE setting, the problem of selecting training data $D$ can be formulated to selecting a design $\mc D = \{ (p_i, \tau_i) \}_{i=1}^n$ of evaluation points and tolerances, whose evaluation through the numerical forward model then provides the training data $D(\mc D) = \{ (p_i, \tau_i, H(u_{\tau_i}(\cdot; p_i))) \}_{i=1}^n$. \newline
As the computational costs of the forward model evaluations depend on the tolerance level and not on the resulting value, we write $W(\mc D)$ for the computational cost for the creation of $D(\mc D)$. \newline
Finally, it useful to talk about refinements of a design: for two designs $\mc D$ and $\mc D'$, we say that $\mc D'$ refines $\mc D$, denoted by $\mc D' \leq \mc D$, if for every $(p, \tau) \in \mc D$ there exists $(p, \tau') \in \mc D'$ such that $\tau' \leq \tau$, i.e.  $\mc D'$ contains all the evaluation points of $\mc D$ with a smaller or equal tolerance.
For $\mc D' \leq \mc D$, we write 
\[ 
W(\mc D' \mid D(\mc D)) \coloneq W(\mc D') - W(\mc D)
\] for the computational cost of computing $D(\mc D')$ having already computed $D(\mc D)$. \medbreak

By using the expected error $E(D)$ as an evaluation metric and the computational budget $W$ as a constraint, we can formulate the following optimal DoE problem:
\begin{equation} \label{prob:doe}
    \min_{\mc D} E(D(\mc D)) \quad \text{subject to} \quad W(\mc D) \le W,
\end{equation}
This problem cannot be solved directly, as the evaluation of $E(D(\mc D))$ requires knowledge of training data $D$ which is not available before the design is fixed and the forward model evaluations are performed.
A variety of approaches to optimal DoE problems exist~\cite{HuanJagalurMarzouk2024} and in the present setting we resort to a greedy sequential approach. 
This approach is based on the idea of incrementally selecting new points and tolerances to update the design by considering the information gathered so far, evaluating the model after each step. \newline
This is a compromise between static designs, which first select the points and then evaluate the model, and Bayesian sequential look-ahead methods, which marginalize on the outcome of future experiments at each step; while the latter can be extremely effective, in our setting the design includes not only the evaluation points but also the tolerance levels, rendering the greedy approach the most suitable. \medbreak

The sequential approach considers a fractionating of the budget $\Delta W_1, \dots, \Delta W_J$ and produces a sequence of designs $\mc D_0,\dots, \mc D_J$, where $\mc D_j$ refines $\mc D_{j-1}$ for every $j \in \{1,\dots, J\}$.
The designs are obtained by solving the following sequence of DoE problems:
\begin{equation} \label{prob:incremental-doe}
    \min_{\mc D_{j} \le \mc D_{j-1}} E(D(\mc D_{j})) \quad \text{s.t.} 
    \quad W(\mc D_{j} \mid D(\mc D_{j-1}) ) \le \Delta W_j, \ \text{ for every } \ j=1,\dots,J.
\end{equation} 
Notice that at each step, Problem~\eqref{prob:incremental-doe} is not yet directly numerically solvable; in fact:
\begin{enumerate}[label=\textbf{\arabic*}]    
    \item $E(D(\mc D_{j}))$ involves the computation of an integral with respect to the unknown posterior $\pi_{Y^m = y^m}$;
    \item the problem is a mixed continuous-combinatorial optimization problem with possibly non convex objective and constraints;
    \item the computational cost associated with a design is not known before the simulations are performed.
\end{enumerate}
These issue are overcome separately in the following ways:
\begin{enumerate}[label=\textbf{\arabic*}]
    \item interleave sampling steps and training steps: sample from the approximated posterior $\pi_{D_{j-1}, Y^m = y^m}$ before step $j$, concatenating the new samples to the chain $\mc S_{j-1}$ and discarding old samples to obtain the current posterior representation $\mc S_j$;
    \item the choice of new evaluation points is separated from the optimization of evaluation accuracies, splitting the problem into two simpler subproblems;
    \item the asymptotical computational effort estimates given by Equation~\eqref{eq:FE-cost} in Section~\ref{sec:AdaFE} are adopted to estimate the computational cost of a design, leading for $\mc D = \{ (p_i, \tau_i) \}_{i=1}^n$ to the estimate
    \begin{equation} \label{eq:comp-cost}
        W(\mc D) = \sum_{i=1}^n W_{\tau_i} = \sum_{i=1}^n \tau_i^{- \frac{l}{r}}.
    \end{equation}
\end{enumerate}

For point \textbf{2}, the selection of new candidate points at step $j$ is based on an acquisition function derived from the expected error $E(D)$
\begin{equation}\label{eq:acq-fun-gen}
    \mc A_j(p) = \int_{\Theta} \alpha_{\mc D_{j-1}}(p, p') \, \pi_{P \mid Y^m = y^m}(p') \, dp',
\end{equation}
where $\alpha_{\mc D_{j-1}}(p, p')$ quantifies the reduction of error in $p'$ when adding $p$ to the design $\mc D_{j-1}$. 
In the following sections, specific versions of $\alpha_{\mc D_{j-1}}$ for GPR and LR surrogates will be described. \newline
The optimization of the evaluation tolerances is performed by solving Problem~\ref{prob:incremental-doe} as a function of the accuracies.
Precisely, at step $j$ let $\mc D_{j-1} = \{ (p_i, \tau_i) \}_{i=1}^{n_{j-1}}$ and new candidates $\{ p_{n+i} \}_{i=1}^{s_j}$; the admissible tolerances for designs that refine $\mc D_{j-1}$ and add new points among the candidates are given by
\begin{equation*}
    \mc T_j =  \Big \{ (\tilde \tau _1, \dots, \tilde \tau _{n_{j-1}+s_j}) \in ( \R ^+ \cup \{ +\infty \} ) ^{n_{j-1}+s_j} \mid \tilde \tau_i  \leq \tau_ i \text{ for } i \leq n_{j-1}\Big \},
\end{equation*}
where for $\tau_i = +\infty$ the corresponding point $p_i$ is excluded from the design.
The tolerance problem at step $j$ is then given by
\begin{equation} \label{prob:tolerance-doe}
    \min_{\mc D_{j} \in \bb D_j} E(D(\mc D_{j})) \quad \text{s.t.} 
    \quad W(\mc D_{j} \mid D(\mc D_{j-1}) ) \le \Delta W_j, 
\end{equation} 
where \[
\bb D_j = \{ \mc D = \{ (p_i, \tau_i) \}_{i=1}^{n_{j-1}+s_j} \mid (\tau_1, \dots, \tau_{n_{j-1}+s_j}) \in \mc T_j \}.
\]
\newpage

The following pseudo-algorithm summarizes the adopted methodology. \medskip

\par\noindent\rule[1mm]{\textwidth}{0.4pt}
\phantomsection \makeatletter\def\@currentlabel{Algorithm 2}\makeatother\label{algo:AL}
\large{\textbf{Algorithm 2 - Adaptive Design of Experiments.} } \normalsize
\par\noindent\rule[2mm]{\textwidth}{0.2pt}
\textbf{Require:} initial design $\mc D_0$ and corresponding training set $D(\mc D_0)$, a tolerance level \texttt{tol}.
\par\noindent\rule[2mm]{\textwidth}{0.2pt}
Initialize sample chain $\mc S_0 = \emptyset$. \newline
Compute initial error level $E(D(\mc D_0))$. \newline
\textbf{Iterative solution of the training problem.}
For $j =1, \dots, J_{\max}$ do: 
\begin{enumerate}
    \item \textbf{Sample the posterior.} \newline
    Decide: $d_j$, $b_j \in \bb N$. \newline
    Sample $d_j$ points from $\pi_{P \mid D(\mc D_{j-1}), Y^m = y^m}$. \newline
    Discard the $b_j$ oldest samples from $\mc S_{j-1}$. \newline
    Concatenate the new samples to the remaining old samples to obtain $\mc S_j$.

    \item \textbf{Select the candidates.} \newline 
    Decide: $s_j \in \bb N$. \newline
    For any $p\in \Theta$, approximate the acquisition function~\ref{eq:acq-fun-gen} by
    \begin{equation}\label{eq:acq-fun-disc}
        \bar{\mc A}_j(p) \coloneq \frac{1}{|\mc S_j|}\sum_{p' \in \mc S_j} \alpha_{\mc D_{j-1}}(p, p') .
    \end{equation}
    Select the $s_j$ best local maximizers of $\bar{\mc A}_j$.

    \item \textbf{Select the tolerances.} \newline
    Decide $\Delta W_j \in \R^+$. \newline
    Solve the discretization of Problem~\ref{prob:tolerance-doe}
    \begin{equation}\label{prob:discrete-tol}       
        \min_{\mc D_{j} \in \bb D_j} \frac{1}{|\mc S_j|} \sum_{p' \in \mc S_j} e_{D(\mc D_j)}(p')  \quad \text{s.t.} 
        \quad W(\mc D_{j} \mid D(\mc D_{j-1}) ) \le \Delta W_j, 
    \end{equation} to obtain the new experimental design $\mc D_j $. \newline
    If $\tau_i = +\infty$ for some $(p_i, \tau_i) \in \mc D_j $, discard $(p_i, \tau_i) $ from $\mc D_j $.

    \item \textbf{Evaluate the model.} \newline
    For every $(p_i, \tau_i )$ in $\mc D_j$: \begin{itemize}[nosep]
        \item if $(p_i, \tau_i ) \notin \mc D_{j-1}$, i.e. it's a new point or the tolerance was updated, set $y_i = H(f_{\tau_i}(\cdot, p_i))$ and include $(p_i, \tau_i, y_i)$ in $D(\mc D_j)$;
        \item else, there exists $(p_i, \tau_i, y_i) \in D(\mc D_{j-1})$: include it in $D(\mc D_j)$.
    \end{itemize}
    Train the surrogate model on $D(\mc D_j)$, obtain $y_{D(\mc D_j)}$.

    \item \textbf{Check for convergence.} \newline
    Compute the new error level $E(D(\mc D_j))$. \newline
    If $E(D(\mc D_j)) \leq \texttt{tol}$, set $J_{\max} = j$ and break the cycle.
\end{enumerate}
\textbf{Final  round of samples.} \newline
Decide: $d_{J_{\max} + 1}$, $h_{J_{\max}+1} \in \bb N$. \newline
Sample $d_{J_{\max}+1}$ points from $\pi_{P \mid D(\mc D_{J_{\max}}), Y^m = y^m}$. \newline
Discard the $h_{J_{\max}+1}$ oldest samples from $\mc S_{J_{\max}}$. \newline
Concatenate the new samples to the remaining old samples to obtain $\mc S_{J_{\max}+1}$. \newline
Return the resulting surrogate model $y_{D(\mc D_J)}$ and posterior samples $\mc S_{J_{\max}+1}$.
\par\noindent\rule[3.5mm]{\textwidth}{0.4pt}

The algorithm depends on a number of parameters, namely the fractionating of the budget $\Delta W_j$, the numbers of samples to draw $d_j$ and to discard $h_j$, and the number of candidates to select $s_j$.
These parameter have to be decided by the user, and Section~\ref{sec:setup} will describe the choices made in this work.

\subsection{Design of Experiments for GPR}\label{sec:GPAL}

For GPR surrogate models, we consider the local error indicator function 
\begin{equation} \label{eq:loc-err-GP}
    e_{D, \text{GPR}}(p) = \text{Trace} (k_D(p,p)),
\end{equation}
which can be interpreted as the expected value over the GP predictive posterior of the squared error of the surrogate model mean at $p$, as
\[
    \text{Trace} (k_D(p,p)) = \bb E_{y(p) \sim \mc N (m_D, k_D(p,p)) } \big[ \| m_{D}(p) - y(p)\|_2^2 \big].
\]
Note that, as the predictive covariance $k_D(p,p)$~\eqref{eq:GP-predictive} does not depend on the model evaluations $y_i$, $e_{D, \text{GPR}}(p)$ can be computed when the design $\mc D$ is known, but the corresponding training set $D(\mc D)$ is not. 
This renders it possible to solve the tolerance problem~\eqref{prob:tolerance-doe} without any other approximation than the discretization of the integral as given in Equation~\eqref{prob:discrete-tol}.  \medbreak

As an acquisition function for new candidates we consider the sensitivity of the error indicator with respect to computational work spent in the point, following~\cite{SemlerWeiser2024,VillaniUngerWeiser2024}.\newline
Assume to be at step $j$ of the algorithm, with a design $\mc D_{j-1} = \{ (p_i, \tau_i) \}_{i=1}^{n_{j-1}}$.
We define the map 
\begin{gather*}
    \Gamma_{\mc D_{j-1}} : \Omega \times \Omega \times \R^+ \to SPSD_{\text{dim}\mc Y}(\R) \\
    (p', p, \tau) \mapsto \text{Trace} \left( k_{D(\mc D_{j-1} \cup \{(p, \tau)\}) }(p',p') \right) 
\end{gather*}
which maps a parameter point $p'$, a candidate point $p$ and a tolerance level $\tau$ to the predictive covariance of the surrogate model $y_{D(\mc D_{j-1} \cup \{(p, \tau)\}) }$ trained on any training data $D(\mc D_{j-1} \cup \{(p, \tau)\})$  resulting from the design $\mc D_{j-1}$ with the addition of the new point $(p, \tau)$.
Note that this map is well-defined as, for any design $\mc D$ and corresponding training data $D(\mc D)$, the predictive covariance $k_{D(\mc D)}$ is actually a function of $\mc D$ only and does not depend on the model evaluations, as noted above.\newline
Then, the sensitivity of the error indicator at $p'$ with respect to the computational work spent in $p$ for an evaluation at a given default tolerance $\tau_p$ is defined by
\begin{equation} \label{eq:alpha-GP}
    \alpha_{\mc D_{j-1}, \text{GPR}}(p, p') = \frac{\partial \Gamma_{\mc D_{j-1}}}{\partial \tau} \bigg |_{(p',p,\tau_p)} \frac{\partial \tau}{\partial W}\bigg |_{W(\tau_p)},
\end{equation}
where $W(\tau)$ is the estimate given by Equation~\eqref{eq:FE-cost} for the computational work needed to evaluate the forward model with tolerance $\tau$ and $\tau(W)$ is the inverse of such relation. \newline
Note that the derivatives are well defined for every $\tau >0$ as long as the discretization noise covariance $\Sigma_d(\tau)$ is differentiable with respect to $\tau$, as in the case given by Equation~\eqref{eq:iid-GPR-noise}. 
For the explicit expressions of the derivatives with the current notation, we refer to the Appendix~\ref{app:derivatives}.

The acquisition function is then given by~\eqref{eq:acq-fun-gen} with $\alpha_{\mc D_{j-1},\text{GPR}}$ as in~\eqref{eq:alpha-GP}. 
It depends on the default tolerance parameter $\tau_p$, which can be either a fixed value or a function of the point $p$; in~\cite{VillaniUngerWeiser2024} it is set to the average over the components of the predictive standard deviation, i.e.
\[ \tau_p = \frac{1}{\text{dim}\mc Y} \sum_{i=1}^{\text{dim}\mc Y} \sqrt{ \big (k_{D(\mc D_{j-1})}(p,p) \big )_{i,i}}. \]  \medbreak

In this work, we further attempt to improve the efficiency of GPR by inferring the noise covariance $\Sigma^2_d(\tau)$ from the evaluations of the forward model.\newline
As the model evaluations are performed through Adaptive FE, for each data point $(p_i, \tau_i, y_i) \in D$ we will have a sequence of estimated residuals 
\begin{equation}\label{eq:eval-seq}
     n_{i,1}, \dots, n_{i,r_i}
\end{equation} 
    corresponding to decreasing estimated error levels 
\begin{equation} \label{eq:tol-seq}
        \tau_{i,1} > \dots > \tau_{i,r_i} = \tau_i
\end{equation}
given by the sequence of FE refinements necessary to obtain $y_i$.
We aim at exploiting this evaluations to estimate the noise covariance $\Sigma^2_d(\tau)$ for the GPR model under certain assumptions by adopting a linear shrinkage estimator~\cite{LedoitWolf2004b}. 
Such an estimator is adopted due to its efficiency in contexts where the amount of data and the dimensionality of the covariance matrix are of comparable sizes. \newline
Given some centered i.i.d. data $x_1,\dots, x_s$ in $\R^d$, a linear shrinkage estimator for the covariance matrix $\Sigma^2 \in \R^{d\times d}$ of the generating distribution is given by the convex combination of the empirical covariance matrix 
\begin{equation}\label{eq:empirical-cov}
    S = \frac{1}{s} \sum_{i=1}^s x_i \cdot x_i^T
\end{equation}
and some other fixed SPD matrix $\Sigma_0$, resulting in an estimator of the form
\begin{equation}\label{eq:shrinkage-estimator-gen}
    \hat \Sigma^2 = \delta \Sigma^2_0 + (1-\delta) S,
\end{equation}
where the parameter $\delta$ is an estimate of the value which minimizes the mean squared error of the estimator and given by 
\begin{equation} \label{eq:shrinkage-delta}
    \delta = \frac{1}{\big \|S - \Sigma^2_0 \big \|_F^2} \min \left( \frac{1}{s^2} \sum_{i=1}^{s} \big \|S - x_i \cdot x_i^T\big \|_F^2, \big \|S - \Sigma^2_0 \big\|_F^2 \right).
\end{equation}\medbreak

In the present setting, we aim at estimating the noise covariance $\Sigma^2_d(\tau)$ of the discretization noise affecting the forward model evaluations at a given tolerance level $\tau$.
We first assume that $\Sigma^2_d(\tau)$ scales quadratically with the tolerance level, i.e. 
\[
\Sigma^2_d(\tau) = \tau^2 \Sigma^2_d(1),
\]
and second we assume that the residual estimates $n_{i,j}$ are faithful realizations of the discretization noise $\nu_d$.
Under these assumptions it holds that 
\[
\tilde n_{i,j} \coloneq \frac{1}{\tau_{i,j}} n_{i,j} \text{ is a realization of } \eta_{i,j} \sim \mc N (0, \Sigma^2_d(1))
\]
for every $ i=1,\dots,n $ and $ j=1,\dots,r_i$. \newline
While assuming $\eta_{i,j}$ to be independent from $\eta_{h,k}$ for $i\neq h$ can be a reasonable approximation, the residuals in a same evaluation sequence will most likely be correlated.
However as we are assuming the data to be centered, the empirical covariance $S$ defined by Equation~\eqref{eq:empirical-cov} will still be an unbiased estimator of the covariance matrix $\Sigma^2_d(1)$ even if less efficient.
Thus, we can use the residuals $\tilde n_{i,j}$ to compute the empirical covariance matrix $S$ of the data $\{\tilde n_{i,j}\}_{i,j}$, which is given by  
\begin{equation}
    \hat S = \frac{1}{n_{\text{tot}}} \sum_{i=1}^n\sum_{j=1}^{r_i} \tilde n_{i,j} \cdot \tilde n_{i,j}^T,
\end{equation} 
where $n_{\text{tot}} = \sum_{i=1}^n r_j $ is the sample size of $\{\tilde  n_{i,j}\}_{i,j}$. \newline
Then, the linear shrinkage estimator for the noise covariance $\Sigma^2_d(1)$ is given by
\[
    \hat \Sigma_d^2 = \delta   I_{\text{dim}\mc Y} + (1-\delta) \hat S,
\]
where we adopted the identity matrix $I_{\text{dim}\mc Y}$ as the fixed matrix $\Sigma_0$ of Equation~\eqref{eq:shrinkage-estimator-gen} and the parameter $\delta$ is given by the formula~\eqref{eq:shrinkage-delta} with the substitution of $\{x_i\}$ by $\{\tilde n_{i,j}\}$, of $S$ by $\hat S$ and of $s$ by $n_{\text{tot}}$.\newline
This leads to the following estimator for the noise covariance $\Sigma^2_d(\tau)$
\begin{equation}\label{eq:shrinkage-estimator}
    \hat{\Sigma}_d^2(\tau) = \tau^2 \hat \Sigma_d^2,
\end{equation}
which can be used when computing the predictive mean and variance of the GPR model as given by Theorem~\ref{thm:GP-posterior}.

\subsection{Design of Experiments for LR}\label{sec:LRAL}

For a LR surrogate model $y_{D,\text{LR}}$, let $\Sigma^2_{y_{D,\text{LR}}}(p)$ be the covariance matrix of the distribution $y_{D,\text{LR}}(p) = \mc U (\mc PI_p)$, where we recall that $\mc PI_p$ is the predictive interval given by Equation~\eqref{eq:LR-PI-general}.\newline
Then, the considered local error indicator function is the vector 1-norm of the square root $\Sigma_{y_{D,\text{LR}}}(p)$ of $\Sigma^2_{y_{D,\text{LR}}}(p)$,
\begin{equation} \label{eq:loc-err-LR-general}
    e_{D, \text{LR}}(p) = \text{trace}(\Sigma_{y_{D,\text{LR}}}(p)).
\end{equation}
In the case of independent discretization noise components described by Proposition~\ref{cor:LR-const}, as the predictive interval given by Equation~\eqref{eq:LR-PI} is
\begin{equation*}
    \mc PI_p = \bigoplus_{j=1}^{\text{dim}\mc Y} \left[ LB^{(j)}(p), UB^{(j)}(p)\right],
\end{equation*}
we have that
\[
    \Sigma^2_{y_{D,\text{LR}}} = \frac{1}{12} \text{diag} \Big ( \big( UB^{(1)}(p) - LB^{(1)}(p)\big)^2, \dots,\big( UB^{(\text{dim}\mc Y)}(p) - LB^{(\text{dim}\mc Y)}(p) \big)^2 \Big),
\] and the local error indicator function can be computed by 
\begin{equation} \label{eq:loc-err-LR}
    e_{D, \text{LR}}(p) = \frac{1}{2\sqrt{3}} \sum_{j=1}^{\text{dim}\mc Y} \left( UB^{(j)}(p) - LB^{(j)}(p) \right).
\end{equation}
Note that neither in the general case~\eqref{eq:loc-err-LR-general} nor in the independent noise case~\eqref{eq:loc-err-LR} the error indicator function can be computed without the knowledge of the training data $D$: this is a significant difference with respect to the GPR case and requires the approximation of the error indicator function in the tolerance problem~\eqref{prob:tolerance-doe}.\medbreak

As an acquisition function for new candidates, we consider the expected error reduction in over the predictive interval $\mc PI_{p}$ for a new point $p$. \newline
Let us be at step $j$ of the algorithm, with a design $\mc D_{j-1} = \{ (p_i, \tau_i) \}_{i=1}^{n_{j-1}}$.
For a point $p \in \Omega$, a tolerance $\tau_p\in \R^+$ and a value $y\in \mc Y$, we write $D_j(p,\tau_p, y) = D(\mc D_{j-1} ) \cup \{(p, \tau_p, y)\}$ for the training data obtained by adding the point $p$ at tolerance $\tau_p$ and value $y$ to the available training data $D(\mc D_{j-1})$. \newline
The expected error reduction in $p'$ when adding $p$ at tolerance $\tau_p$ to the design $\mc D_{j-1}$ is then given by the expected value over the model response $Y_p$ and the discretization noise $\nu$ of the difference between the current error indicator and the error indicator resulting from training set $D_j(p,\tau_p, y)$:
\begin{equation}\label{eq:alpha-LR-general}
    \alpha_{\mc D_{j-1}, \text{LR}}(p, p') = 
    \bb E _{ (Y_p, \nu) \sim \mc U\left( \mc PI_p \times  [-\tau_p, \tau_p] ^{ \text{dim} \mc Y } \right)} 
    \left[ 
        e_{D(\mc D_{j-1}), \text{LR}}(p')- e_{D_j(p,\tau_p, Y_p + \nu ), \text{LR}}(p')
    \right].
\end{equation}
Note that $\alpha_{\mc D_{j-1}, \text{LR}}$ is always non negative, as the predictive interval $\mc PI_{p'}$ can only shrink when adding a new point to the design and $e_{D, \text{LR}}(p)$ decreases as the size of $\mc PI_{p'}$ decreases. \medbreak

For the independent noise case, we can explicitly compute the expected value in~\eqref{eq:alpha-LR}, obtaining a closed-form expression for $\alpha_{\mc D_{j-1}, \text{LR}}$ given by the following proposition.
\begin{restatable}[Expected error reduction]{prp}{EER} \label{prp:EER}
    Let $p, p' \in \Omega$ and $\tau_p \in \R^+$, let $D (\mc D)= \{ (p_i, \tau_i, y_i) \}_{i=1}^n$ be a training set resulting from the design $\mc D$, and let the hypothesis of Proposition~\ref{prp:LR-PI} hold. \newline
    Let $L^{(1)}, \dots L^{(\text{dim} \mc Y)}$ be the Lipschitz constants used in the LR surrogate.
    Then, the expected error reduction in $p'$ is given by \begin{equation}\label{eq:alpha-LR}
        \alpha_{\mc D, \text{LR}}(p, p') =  \frac{1}{4 \sqrt{3} \tau_p } \sum_{j=1}^{\text{dim}\mc Y} \frac{EUI^{(j)}(p,p') - ELI^{(j)}(p,p')}{UB^{(j)}(p) - LB^{(j)}(p)} ,
    \end{equation}
    where $m_{\mc Y}$ is the Lebesgue measure over $\mc Y$ and the expected upper improvement $EUI$ and expected lower improvement $ELI$ in the $j$-th component are given by
    \begin{equation*}
        \makebox[\textwidth][c]{$
        EUI^{(j)}(p,p')  = 
        \begin{cases}
            \tau_p\left(UB^{(j)}(p) -LB^{(j)}(p)\right)\left( 2 c_1^j  - UB^{(j)}(p) - LB^{(j)}(p)\right) &
            \text{if } \ UB^{(j)}(p) \leq c_1^j - \tau_p  
            \\[3ex]

            \dfrac{1}{6}\left(\left(c_1^j - LB^{(j)}(p) + \tau_p\right)^3 - \left(c_1^j - LB^{(j)}(p) - \tau_p\right)^3\right)&
            \begin{aligned}
                \text{if } & \ c_1^j + \tau_p < UB^{(j)}(p) \\
                &\text{ and } LB^{(j)}(p) \leq c_1^j - \tau_p
            \end{aligned}
            \\[4ex]

            \dfrac{1}{6}\left(c_1^j+\tau_p-LB^{(j)}(p)\right)^3 &
            \begin{aligned}
                \text{if } & \ c_1^j + \tau_p < UB^{(j)}(p) \\
                &\text{ and } c_1^j - \tau_p < LB^{(j)}(p) \leq c_1^j + \tau_p
            \end{aligned}
            \\[4ex]

            \dfrac{1}{6}\left(\left(c_1^j - LB^{(j)}(p) + \tau_p\right)^3 - \left(c_1^j - UB^{(j)}(p) + \tau_p\right)^3\right) &
            \begin{aligned}
                \text{if } & \ c_1^j - \tau_p < UB^{(j)}(p) \leq c_1^j + \tau_p \\
                &\text{ and } c_1^j - \tau_p < LB^{(j)}(p) \leq c_1^j + \tau_p
            \end{aligned}
            \\[4ex]

            \begin{aligned}
                2\tau_p\bigg(c_1^j \left(c_1^j - \tau_p -LB^{(j)}(p)\right) &+ \dfrac{1}{2} \left(LB^{(j)}(p)^2 - \left(c_1^j-\tau_p\right)^2\right)\bigg) -\\
                &- \dfrac{1}{6} \left( \left(c_1^j -UB^{(j)}(p) + \tau_p\right)^3 -8\tau_p^3\right)
            \end{aligned}&
            \begin{aligned}
                \text{if } & \ c_1^j - \tau_p < UB^{(j)}(p) \leq c_1^j + \tau_p \\
                &\text{ and } LB^{(j)}(p) \leq c_1^j - \tau_p
            \end{aligned}
            \\[5ex]
            0 & 
            \text{if } \  c_1^j + \tau_p  < LB^{(j)}(p)
        \end{cases}$
        }
    \end{equation*}
    and 
    \begin{equation*}
        \makebox[\textwidth][c]{$
        ELI^{(j)}(p,p')  = 
        \begin{cases}
            \tau_p \left(UB^{(j)}(p) -LB^{(j)}(p)\right) \left(- 2 c_2^j - UB^{(j)}(p) - LB^{(j)}(p)\right) &
            \text{if } \ LB^{(j)}(p) \geq \tau_p - c_2^j  
            \\[3ex]

            \dfrac{1}{6}\left(\left(c_2^j + UB^{(j)}(p) - \tau_p\right)^3 - \left(c_2^j + LB^{(j)}(p) + \tau_p\right)^3\right)&
            \begin{aligned}
                \text{if } & \ - c_2^j - \tau_p > LB^{(j)}(p) \\
                &\text{ and } UB^{(j)}(p) \geq \tau_p - c_2^j
            \end{aligned}
            \\[4ex]

            -\dfrac{1}{6}\left(c_2^j+\tau_p+UB^{(j)}(p)\right)^3 &
            \begin{aligned}
                \text{if } & \ - c_2^j - \tau_p > LB^{(j)}(p) \\
                &\text{ and } \tau_p - c_2^j > UB^{(j)}(p) \geq - c_2^j - \tau_p
            \end{aligned}
            \\[4ex]

            \dfrac{1}{6}\left(\left(c_2^j + LB^{(j)}(p) + \tau_p\right)^3 - \left(c_2^j + UB^{(j)}(p) + \tau_p\right)^3\right) &
            \begin{aligned}
                \text{if } & \ \tau_p - c_2^j > LB^{(j)}(p) \geq - c_2^j - \tau_p \\
                &\text{ and } \tau_p - c_2^j > UB^{(j)}(p) \geq - c_2^j - \tau_p
            \end{aligned}
            \\[4ex]

            \begin{aligned}
                2\tau_p\bigg(c_2^j \left(-c_2^j + \tau_p -UB^{(j)}(p)\right) &- \dfrac{1}{2} \left(UB^{(j)}(p)^2 - \left(c_2^j-\tau_p\right)^2\right)\bigg) +\\
                &+ \dfrac{1}{6} \left( \left(c_2^j +LB^{(j)}(p) + \tau_p\right)^3 -8\tau_p^3\right)
            \end{aligned}&
            \begin{aligned}
                \text{if } & \ \tau_p -c_2^j > LB^{(j)}(p) \geq - c_2^j - \tau_p \\
                &\text{ and } UB^{(j)}(p) \geq \tau_p - c_2^j
            \end{aligned}
            \\[5ex]
            0 & 
            \text{if } \  -c_2^j - \tau_p  > UB^{(j)}(p)
        \end{cases}$
        }
    \end{equation*}
    for $c_1^j = UB^{(j)}(p') - L^{(j)} \| p - p' \|_\Omega -\tau_p$ and $c_2^j = -LB^{(j)}(p') - L^{(j)} \| p - p' \|_\Omega -\tau_p$.
\end{restatable}
For the sake of brevity the proof of this result is given in the Appendix~\ref{app:EER}, as it involves the integration of a piecewise linear function and needs to distinguish between different cases. \medbreak

With the expected error reduction $\alpha_{D, \text{LR}}$ at hand, the acquisition function for new candidates is given by~\eqref{eq:acq-fun-gen}. 
Like for the GPR acquisition function, it depends on the default tolerance parameter $\tau_p$, which can be treated as in the GPR case. \medbreak

As mentioned with the introduction of the LR error indicator function $e_{D, \text{LR}}$ in~\eqref{eq:loc-err-LR}, this function unlike $e_{D, \text{GPR}}$ cannot be computed without the knowledge of the training data $D$; thus, in the tolerance problem~\eqref{prob:tolerance-doe} $e_{D, \text{LR}}$ cannot be directly used. 
Moreover, as the Lipschitz constants $L^{(1)}, \dots, L^{(\text{dim} \mc Y)}$ are estimated via~\eqref{eq:lips-const}, they depend both on the training data $D$ and on the tolerance levels $\tau_1, \dots, \tau_n$. \newline
The handling of the first problem requires more care and we'll dedicate the remainder of the section to its solution; the second problem instead is solved by fixing the Lipschitz constants to \[
    L^{(j)} = \max_{ \stackrel{i, k \in \{1, \dots, n\}}{i\neq k} } \frac{| y_i^{(j)} - y_k^{(j)} |}{\| p_i - p_k \|_\Omega},
\]
which is obtained from Equation~\eqref{eq:LR-const} by setting $\tau_i=0$ for every $i=1,\dots,n$.
Such assignment makes the constants independent from the tolerance levels $\tau_1, \dots, \tau_n$ and guarantees that the Lipschitz conditions are compatible with any tolerance level.\medskip

The derivation of a computable tolerance problem for LR follows a similar idea as done for the acquisition function~\eqref{eq:alpha-LR-general}.
For a fixed set of evaluation points $p_1, \dots, p_n$ with predictive intervals $\mc PI_{p_1}, \dots, \mc PI_{p_n}$ we consider the expected value of the error indicator function over the model response and the discretization noise
\begin{equation} \label{eq:tol-exp-err}
    E(p, \tau_1, \dots, \tau_n)\coloneq \bb E _{ (Y_{p_1}, \dots Y_{p_n}, \nu_1, \dots, \nu_n) \sim  \mc U\left( \bigoplus_{i=1}^n \mc PI_{p_i} \times \bigoplus_{i=1}^n  [-\tau_i, \tau_i] ^{ \text{dim} \mc Y } \right)} 
    \left[ 
         e_{\{(p_i,\tau_i, Y_{p_i} + \nu_i )\}_{i=1}^n, \text{LR}}(p)
    \right]
\end{equation}
as a function the given tolerance levels $\tau_1, \dots, \tau_n$

Computing analytically the integrals is challenging due to the multiple cases to be considered, and a recursive formula on $n$ could not be found. 
Consequently, an approximation of this expected value is required; as Monte Carlo integration would pose some computational challenges due to the dimensionality of the problem growing with the number of evaluation points $n$, we look for some bounds which can be used to estimate $E(\tau_1,\dots, \tau_n)$.\newline
We once again consider the independent noise component setting to derive a computable quantity: in such case it holds
\begin{gather*}
    e_{\{(p_i,\tau_{i}, Y_{p_i} + \nu_i )\}_{i=1}^n, \text{LR}}(p)  = 
    \frac{1}{2\sqrt{3}} \sum_{j=1}^{\text{dim}\mc Y} \left( UB^{(j)}(p) - LB^{(j)}(p) \right) = \qquad \qquad \qquad \qquad \qquad\qquad \qquad\qquad \qquad  \\
    \qquad = \frac{1}{2\sqrt{3}} \sum_{j=1}^{\text{dim}\mc Y} 
    \left( \min_{i=1,\dots,n}\left\{ Y_{p_i} + \nu_i + \tau_{i}^{(j)} + L^{(j)} \| p - p_i \|_\Theta \right\}- \max_{i=1,\dots,n} \left\{ Y_{p_i} + \nu_i - \tau_i^{(j)} - L^{(j)} \| p - p_i \|_\Theta \right\} \right),
\end{gather*} 
where the first equality is given by Equation~\eqref{eq:loc-err-LR} and in the second we substituted to $LB^{(j)}(p)$ and $UB^{(j)}(p)$ their formulas as given by Equations~\eqref{eq:LR-bounds}.\newline
Due to the monotonicity of the expected value, the expected value of the minimum of $n$ random variables is less or equal to the minimum of the expected values, and similarly for the maximum. 
By this, it holds that 
\begin{align*}
    E(p, \tau_1, \dots, \tau_n) &=\bb E\left[ 
        e_{\{(p_i,\tau_{i}, Y_{p_i} + \nu_i )\}_{i=1}^n, \text{LR}}(p)
   \right] \leq\\
   &\leq \frac{1}{2\sqrt{3}} \sum_{j=1}^{\text{dim}\mc Y} 
    \Big( \min_{i=1,\dots,n}\left\{ \bb E[Y_{p_i} + \nu_i] + \tau_i^{(j)} + L^{(j)} \| p - p_i \|_\Theta \right\}- \\
    & \qquad\qquad \qquad\qquad\qquad \qquad \qquad -\max_{i=1,\dots,n} \left\{ \bb E[Y_{p_i} + \nu_i] - \tau_i^{(j)} - L^{(j)} \| p - p_i \|_\Theta \right\} \Big) = \\
    &= \frac{1}{2\sqrt{3}} \sum_{j=1}^{\text{dim}\mc Y} \Big(  \min_{i=1,\dots, n}\left\{ \bb E[Y_{p_i}] + \tau_i^{(j)} + L^{(j)} \| p - p_i \|_\Theta \right\}- \\
    & \qquad\qquad\qquad \qquad\qquad\qquad \qquad \qquad - \max_{i=1,\dots,n} \left\{ \bb E[Y_{p_i} ] - \tau_i^{(j)} - L^{(j)} \| p - p_i \|_\Theta \right\} \Big)\eqcolon \\ 
    & \qquad\qquad \qquad\qquad\qquad \qquad \qquad  \qquad\qquad \qquad\qquad\qquad \qquad \qquad 
     \eqcolon\hat E(p, \tau_1, \dots, \tau_n),
\end{align*}
where the expected values are to be intended as in Equation~\eqref{eq:tol-exp-err}, and in the last equality we used the linearity of the expected value and that the noise $\nu_i$ has zero mean. \medbreak

In the LR training strategy, for any design $\mc D_j = \{ (p_i, \tau_i) \}_{i=1}^{n_j}$ we adopt $\hat{E} (p, \tau_1, \dots, \tau_{n_j})$ as an estimate of $e_{D(\mc D_j), \text{LR}}(p)$ in the tolerance problem~\eqref{prob:tolerance-doe}.
Then, the discrete tolerance problem for LR becomes
\begin{equation}\label{prob:discrete-tol-LR}       
    \min_{\mc D_{j} \in \bb D_j} \frac{1}{|\mc S_j|} \sum_{p' \in \mc S_j} \hat E(p', \tau_1, \dots, \tau_{n_j})  \quad \text{s.t.} 
    \quad W(\mc D_{j} \mid D(\mc D_{j-1}) ) \le \Delta W_j, 
\end{equation} 
which is used to optimize the evaluation tolerances in~\ref{algo:AL} for the LR surrogate model. \newline