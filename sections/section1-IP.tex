\section{Inverse Problems} \label{sec:IP}
Inverse Problems (IP) deal with the identification of some unknown parameter or function in a model through observations of the portion of reality that the model is intended to represent. \newline
A first approach to IP is that of Classical Inverse Problems, where the problem is treated in a deterministic framework, questions of well-posedness are tackled, and the solution is a point estimate of the unknown. 
This framework can be expanded to a statistical approach to IP, where stochastic behavior of the observations is taken into consideration and some probabilistic tools such as the parameters' likelihood are introduced.
Finally, in the Bayesian approach to IP the problem's unknowns are assumed to have random behavior, and probabilistic tools are utilized intensively. 
In the Bayesian treatment, the problem is usually well-posed, but its solution is a probability distribution, requiring a more careful and complex numerical handling.\medskip

The main reference for this section is Tim Sullivan's Introduction to Uncertainty Quantification~\cite{Sullivan2015}, and other sources will be quoted when utilized. \newline
In the remainder of this section, Section~\ref{sec:IP-intro} introduces the basic concepts of Inverse Problems, providing a brief overview of the classical approach and the statistical approach to IP; Section~\ref{sec:BIP} introduces the Bayesian approach to IP, providing a general framework for the formulation of Bayesian Inverse Problems (BIPs) and the derivation of the posterior distribution of the unknown given the observations; finally, Section~\ref{sec:IP-sol} introduces some key techniques for the solution of IPs, with a focus on the Bayesian approach. 

\subsection{Introduction to Inverse Problems}\label{sec:IP-intro}

Inverse Problems deal with the problem of identifying the pre-image of an observation $y^m$ under a map \[y : \Theta \longrightarrow \mc Y \] known as the forward model; $\Theta$ is known as the parameter space and $\mc Y$ is the measurement or observation space.
If a unique $p*$ in $\Theta$ such that
\begin{equation}\label{eq:IP0}
    y(p*) = y^m
\end{equation}
exists, the IP admits a unique solution: but this case is an exception rather than a rule, especially when the observation $y^m$ is corrupted by noise.

A number of techniques have been developed to address the numerous issues which arise in an IP.
First, the problem can be reformulated as a least squares problem: for some norm $\| \cdot \| _\mc Y$ on $\mc Y$, Problem~\ref{eq:IP0} is generalized to a minimization problem
\begin{equation}\label{eq:IP1}
    \min_{p\in \Theta} \| y(p) - y^m \|_\mc Y.
\end{equation}
Under regularity conditions such as $\Theta$ and $\mc Y$ being Banach spaces and $y$ being a continuous map, this formulation cannot yet guarantee neither existence, nor unicity, nor stability of a solution $p*$ for every $y^m \in \mc Y$. Nonetheless, Problem~\ref{eq:IP1} is more general as it allows for solutions even if $y^{-1}(\{y^m\} )= \emptyset$, and on the practical side suggests the adoption of optimization techniques to solve IPs.

A second and often compatible approach is that of regularization techniques. This involves introducing additional information or constraints to stabilize the solution and make the problem well-posed. Regularization can be performed by the substitution of $y$ with some more treatable operator, or in the case of Variational Regularization by the formulation of a different minimization problem. Often, a regularization approach can be understood both from an operator approximation and a variational perspective: this applies to Tychonoff regularization, which can be seen as the addition of a stabilizing term to Problem~\ref{eq:IP1}
\begin{equation}\label{eq:Tycho}
    \min_{p\in \Theta} \| y(p) - y^m \|_\mc Y^2 + \lambda\| p - p^0 \|_\Theta^2,
\end{equation}
for some norm $\|\cdot\|_\Theta$ on $\Theta$, $p^0$ in $\Theta$ and $\lambda $ in $ \R^+$. \medskip


Statistical IPs, likelihood and LS 

\begin{equation}\label{eq:par-to-obs}
    Y^m = y(p) + N
\end{equation}

\begin{equation}\label{eq:likelihood}
    L(p) = \pi_N(y^m - y(p))
\end{equation}

\subsection{Bayesian Inverse Problems}\label{sec:BIP}

A natural development of the statistical approach to Inverse Problems are Bayesian Inverse Problems (BIPs). 
In the Bayesian setting, the unknown solution $p*$ of the IP is treated as a random variable $P$ and the solution of the IP then becomes the conditional distribution $\bb P_{P \mid Y^m=y^m} $ of $P$ given $Y^m=y^m$.

In full generality, the following result guarantees the possibility of formulating BIPs for arbitrary dimensionality:
\begin{thm} [Regular conditional probability]
    Let $ (\Omega, \mc F , \bb P) $ be a probability space, $\Theta$ be a separable Banach space equipped with the Borel $\sigma$-algebra $\mc B (\Theta)$, $(\mc Y, \tilde{\mc F})$ be a measurable space.
    Further, let $Y^m:\Omega \rightarrow \mc Y$ and $P : \Omega \rightarrow \Theta$ be random variables with $P \in L^1(\Omega, \bb P; \Theta) $. \newline
    Then there exists a $\bb P_{Y^m}$-a.s. unique map $\bb P_{P \mid Y^m} : \mc B (\Theta) \times \mc Y \rightarrow [0,1] $ such that :
    \begin{itemize}
        \item $\bb P_{P \mid Y^m}(\cdot, y)$ is a probability density on $\Theta$ for all $y$ in $\mc Y$;
        \item $\bb P_{P \mid Y^m}(A, \cdot)$ is measurable for all $A$ in $\mc B (\Theta)$;
        \item for all $B$ in $\sigma(Y^m)$, $A$ in $\mc B (\Theta)$, it holds that
                \[ 
                \int_B \bb P_{P \mid Y^m}(A, Y^m(\omega)) \ d\bb P(\omega)= \int_B \ind_A(P(\omega)) \ d\bb P(\omega).
                \] 
    \end{itemize}
    Such map is known as the \textbf{regular conditional probability} of $P$ given $Y^m$.
\end{thm}

This results guarantees the well-definiteness of conditional probabilities in a general setting, thus allowing for the formulation of BIPs in arbitrary dimensionality, but does not provide a direct way to formulate the posterior distribution given a likelihood. 
This is provided by Bayes' rule, which in generality is given by the following result from~\cite[Theorem 14]{DashtiStuart2017}:

\begin{thm}[Bayes' rule]
    Let $ (\Omega, \mc F , \bb P) $ be a probability space and $\Theta, \mc Y$ be separable Banach spaces equipped with the respective Borel $\sigma$-algebras. 
    Moreover, let $P : \Omega \rightarrow \Theta$ and $N:\Omega \rightarrow \mc Y$ be independent random variables, and $ Y^m = y(P) + N$ with $y: \Theta \rightarrow \mc Y$ a measurable map. \newline
    Assume $\bb P_{Y^m\mid P}(\cdot, p) \ll P_N$ for every $p \in \Theta$, that $\frac{d \bb P_{Y^m\mid P}(\cdot, p)}{d\bb P_N}(y) $ is $\bb P_{(P,N)}$-measurable, and that 
    \[
        \int_\Theta \frac{d \bb P_{Y^m\mid P}(\cdot, p)}{d\bb P_N}(y) \ d\bb P_P(p) > 0 \ \text{ for }  \ y, \ \bb P_N \text{-a.s.}
    \]
    holds.
    Then, the regular conditional probability $\bb P_{P\mid Y^m}(\cdot,y)$ for $P$ given $Y^m$ exists and is such that $\bb P_{P\mid Y^m}(\cdot, y) \ll \bb P _P$ $\bb P_{Y^m}$-a.s., with Radon-Nikodym derivative
    \begin{equation}\label{eq:infdimBayes}
        \frac{d\bb P_{P\mid Y^m}(\cdot, y)}{d\bb P_P}(p) = \frac{1}{Z(y)}\frac{d\bb P_{Y^m\mid P}(\cdot, p)}{d\bb P_{N}}(y).
    \end{equation}
\end{thm}

In the above theorem, $\bb P_P$ is the prior distribution of $P$ and $\bb P_{P \mid Y^m}(\cdot, y^m)$ is then the Bayesian posterior distribution of $P$ given $Y^m=y^m$.
As in~\cite[Theorem 6.31]{Stuart2010}, under certain hypothesis over the measurement space $\mc Y$, the distribution of the noise $\bb P_N$ and the forward model $y$, one has that the conditions over $\frac{d \bb P_{Y^m\mid P}(\cdot, p)}{d\bb P_N}(y)$ hold, rendering the application of the Theorem possible. \medskip

As for the scope of this work it is not necessary to work in full generality, from now on it will be assumed that the involved spaces $\Theta, \  \mc Y$ are finite dimensional Banach spaces, and that $P$ and $Y$ are random vectors that admit a joint probability density function $\pi_{P,Y}$ with respect to the Lebesgue product measure $m_\Theta \otimes m_\mc Y$. This allows for a more intuitive and direct formulation of the Bayesian posterior distribution of $P$ given $Y=y^m$ by exploiting the conditional probability density.

\begin{thm}[Bayes' rule with Lebesgue measure]
    Let $Y = y(P) + N$ hold, with $P$ and $N$ independent and $y: \Theta \rightarrow \mc Y$ a measurable map.
    Then the likelihood is \[
        \pi_{Y^m\mid P = p}(y^m) = \pi_{N}(y^m - y( p) )
    \] posterior distribution of $P$ given $Y^m=y^m$ is given by the probability density \begin{equation}\label{eq:Bayes}
        \pi_{P\mid Y^m = y^m}(p) = \frac{\pi_{N}(y^m - y( p) ) \pi_P(p)}{\int_\Theta  \pi_{N}(y^m - y( p) ) \pi_P(p) \ dm_\Theta(p)},
    \end{equation}
\end{thm}

\subsection{Solution of Inverse Problems}\label{sec:IP-sol}
ML/MAP\medskip

MCMC (mention effective size sampling)\medskip
