\section{Inverse Problems} \label{sec:IP}
Inverse Problems (IP) deal with the identification of some unknown parameter or function in a model through observations of the portion of reality that the model is intended to represent. \\
A first approach to IP is that of Classical Inverse Problems, where the problem is treated in a deterministic framework, questions of well-posedness are tackled, and the solution is a point estimate of the unknown.
Another possibility is given by the Bayesian approach to IP: the problem is treated as a statistical problem where unknowns are assumed to have random behaviour, and probabilistic tools are utilized intensively. 
In the Bayesian treatment, the problem is usually well-posed, but its solution is a probability distribution requiring a more careful and complex numerical handling.\\
In this section, we will introduce Inverse Problem by first treating briefly their classical formulation and then presenting the Bayesian formulation, which will be adopted throughout the rest of this thesis.

\subsection{Classical Inverse Problems}\label{sec:CIP}

Classical Inverse Problems deal with the problem of identifying the pre-image of an observation $y^m$ under a map \[y : \mc P \longrightarrow \mc Y \] known as the forward model. 
If a unique $p*$ in $\mc P$ such that
\begin{equation}\label{eq:IP0}
    y(p*) = y^m
\end{equation}
exists, the IP admits a unique solution: but this case is an exception rather than a rule, expecially when the observation $y^m$ is corrupted by noise.

A number of techniques have been developed to address the numerous issues which arise in an IP.
First, the problem can be reformulated as a least squares problem: for some norm $\| \cdot \| _\mc Y$ on $\mc Y$, Problem~\ref{eq:IP0} is generalized to a minimization problem
\begin{equation}\label{eq:IP1}
    \min_{p\in \mc P} \| y(p) - y^m \|_\mc Y.
\end{equation}
Under regularity conditions such as $\mc P$ and $\mc Y$ being Banach spaces and $y$ being a continuous map, this formulation cannot yet guarantee neither existance, nor unicity, nor stability of a solution $p*$ for every $y^m \in \mc Y$. Nonetheless, Problem~\ref{eq:IP1} is more general as it allows for solutions even if $y(\{y^m\} )= \emptyset$, and on the practical side suggests the adoption of optimization techniques to solve IPs.

A second and often compatible approach is that of regularization techniques. This involves introducing additional information or constraints to stabilize the solution and make the problem well-posed. Regularization can be performed by the substitution of $y$ with some more treatable operator, or in the case of Variational Regularization by the formulation of a different minimization problem. Often, a regularization approach can be understood both from an operator approximation and a variational perspective: this applies to Tychonoff regularization, which can be seen as the addition of a stabilizing term to Problem~\ref{eq:IP1}
\begin{equation}\label{eq:Tycho}
    \min_{p\in \mc P} \| y(p) - y^m \|_\mc Y^2 + \lambda\| p - p^0 \|_\mc P^2,
\end{equation}
for some norm $\|\cdot\|_\mc P$ on $\mc P$, $p^0$ in $\mc P$ and $\lambda $ in $ \R^+$.

What well-posedness means

Linear case, pseudo-inverses

Regularization

Statistical IPs, likelihood and LS 


\subsection{Bayesian Inverse Problems}\label{sec:BIP}

A natural developement of the statistical approach to Inverse Problems are Bayesian Inverse Problems (BIPs). 
In the Bayesian setting, the unknown solution $p*$ of the IP is treated as a random variable $P$ and the solution of the IP then becomes the conditional distribution $\bb P_{P \mid Y=y^m} $ of $P$ given $Y=y^m$.

In full generality, the following result guarantees the possibility of formulating BIPs for arbitrary dimensionality:
\begin{thm}
    Let $ (\Omega, \mc F , \bb P) $ be a probability space, $\mc P$ be a separable Banach space equipped with the Borel $\sigma$-algebra $\mc B (\mc P)$, $(\mc Y, \tilde{\mc F})$ be a measurable space.
    Moreover let $Y:\Omega \rightarrow \mc Y$ and $P : \Omega \rightarrow \mc P$ be random variables with $P \in L^1(\Omega, \bb P; \mc P) $. \\
    Then there exists a $\bb P_Y$-a.s. unique map $\bb P_{P \mid Y} : \mc B (\mc P) \times \mc Y \rightarrow [0,1] $ such that :
    \begin{itemize}
        \item $\bb P_{P \mid Y}(\cdot, y)$ is a probability density on $\mc P$ for all $y$ in $\mc Y$;
        \item $\bb P_{P \mid Y}(A, \cdot)$ is measurable for all $A$ in $\mc B (\mc P)$;
        \item for all $B$ in $\sigma(Y)$, $A$ in $\mc B (\mc P)$, it holds that
                \[ 
                \int_B \bb P_{P \mid Y}(A, Y) \ d\bb P= \int_B \ind_A(P) \ d\bb P .
                \] 
    \end{itemize}
    Such map is known as the \textbf{regular conditional probability} of $P$ given $Y$.
\end{thm}

In the hypothesis of the above theorem, $\bb P_P$ can be interpreted as a prior distribution of $P$ and $\bb P_{P \mid Y}(\cdot, y^m)$ is then the Bayesian posterior distribution of $P$ given $Y=y^m$.

As for the scope of this work it is not necessary to work in full generality, from now on it will be assumed that the involved spaces $\mc P, \  \mc Y$ are finite dimensional Banach spaces, and that $P$ and $Y$ admit a joint probability density function $\pi_{P,Y}$ with respect to the Lebesgue product measure $m_\mc P \otimes m_\mc Y$.