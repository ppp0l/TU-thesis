\section{Inverse Problems} \label{sec:IP}
Inverse Problems (IP) deal with the identification of some unknown parameter or function in a model through observations of the portion of reality that the model is intended to represent. \\
A first approach to IP is that of Classical Inverse Problems, where the problem is treated in a deterministic framework, questions of well-posedness are tackled, and the solution is a point estimate of the unknown.
Another possibility is given by the Bayesian approach to IP: the problem is treated as a statistical problem where unknowns are assumed to have random behaviour, and probabilistic tools are utilized intensively. 
In the Bayesian treatment, the problem is usually well-posed, but its solution is a probability distribution requiring a more careful and complex numerical handling.\\
In this section, we will introduce Inverse Problem by first treating briefly their classical formulation and then presenting the Bayesian formulation, which will be adopted throughout the rest of this thesis.

\subsection{Classical Inverse Problems}\label{sec:CIP}

Classical Inverse Problems deal with the problem of identifying the pre-image of an observation $y^m$ under a map \[y : \mc P \longrightarrow \mc Y \] known as the forward model. 
If a unique $p*$ in $\mc P$ such that
\begin{equation}\label{eq:IP0}
    y(p*) = y^m
\end{equation}
exists, the IP is well-posed: but this case is an exception rather than a rule, expecially when the observation $y^m$ is corrupted by noise.

A number of techniques have been developed to address the numerous issues which arise in an IP.
First, the problem can be reformulated as a least squares problem: for some norm $\| \cdot \| _\mc Y$ on $\mc Y$, Problem~\ref{eq:IP0} is generalized to a minimization problem
\begin{equation}\label{eq:IP1}
    \min_{p\in \mc P} \| y(p) - y^m \|_\mc Y.
\end{equation}
Under regularity conditions such as $\mc P$ and $\mc Y$ being Banach spaces and $y$ being a continuous map, this formulation cannot yet guarantee neither existance, nor unicity, nor stability of a solution $p*$ for every $y^m \in \mc Y$. Nonetheless, Problem~\ref{eq:IP1} is more general as it allows for solutions even if $y(\{y^m\} )= \emptyset$, and on the practical side suggests the adoption of optimization techniques to solve IPs.

A second and often compatible approach is that of regularization techniques. This involves introducing additional information or constraints to stabilize the solution and make the problem well-posed. Regularization can be understood as the substitution of $y$ with some more treatable approximation operator, but can sometimes be translated into the formulation of a different least-squares problem. This is the case of Tychonoff regularization, which can be framed as the addition of a stabilizing term to Problem~\ref{eq:IP1}
\begin{equation}\label{eq:Tycho}
    \min_{p\in \mc P} \| y(p) - y^m \|_\mc Y + \lambda\| p - p^0 \|_\mc P,
\end{equation}
for some norm $\|\cdot\|_\mc P$ on $\mc P$, $p^0$ in $\mc P$ and $\lambda \in \R^+$.


\subsection{Bayesian Inverse Problems}\label{sec:BIP}