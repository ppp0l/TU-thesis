\section{Preliminaries} \label{sec:preliminaries}

In the remainder of this section Section~\ref{sec:BIP} first introduces the basic concepts of Inverse Problems and then presents the Bayesian approach to IP, providing a general framework for the formulation of Bayesian Inverse Problems (BIPs) and the derivation of the posterior distribution of the unknown given the observations; Section~\ref{sec:IP-sol} presents some key techniques for the solution of IPs and their numerical treatment, with a focus on the Bayesian approach; Section~\ref{sec:PDE} introduces the basic concepts of Partial Differential Equation (PDE) models, with a focus on the Laplace equation, the diffusion equation and elastomechanic equations as they are the PDEs used in the numerical experiments of Section~\ref{sec:exp}; finally, Section~\ref{sec:AdaFE} presents the Finite Element (FE) method and its adaptive version, which is the solution method for PDEs considered throughout this work.

\subsection{Bayesian Inverse Problems}\label{sec:BIP}

The main reference for this section and the following is Tim Sullivan's Introduction to Uncertainty Quantification~\cite{Sullivan2015}, and other sources will be quoted when utilized. \medskip

Inverse Problems (IP) deal with the identification of some unknown parameter or function in a model through observations of the portion of reality that the model is intended to represent.\newline
Given a measurement $y^m$ in the measurement space $\mc Y$, also known as observation space, the goal of an IP is to identify the pre-image $p*$ in the parameter space $\Theta$ under a map \[y : \Theta \longrightarrow \mc Y \] known as the forward model or parameter-to-observation map.
If a unique $p*$ in $\Theta$ such that
\begin{equation}\label{eq:IP0}
    y(p*) = y^m
\end{equation}
exists, the IP admits a unique solution: but this case is an exception rather than a rule, especially when the observation $y^m$ is corrupted by noise.

A number of techniques have been developed to address from an analytical perspective the numerous issues which arise in an IP.
First, the problem can be reformulated as a least squares problem: for some norm $\| \cdot \| _\mc Y$ on $\mc Y$, Problem~\ref{eq:IP0} is generalized to a minimization problem
\begin{equation}\label{eq:IP1}
    \min_{p\in \Theta} \| y(p) - y^m \|_\mc Y.
\end{equation}
Under regularity conditions such as $\Theta$ and $\mc Y$ being Banach spaces and $y$ being a continuous map, this formulation cannot yet guarantee neither existence, nor unicity, nor stability of a solution $p*$ for every $y^m \in \mc Y$. Nonetheless, Problem~\ref{eq:IP1} is more general as it allows for solutions even if $y^{-1}(\{y^m\} )= \emptyset$, and on the practical side suggests the adoption of optimization techniques to solve IPs.

A second and often compatible approach is that of regularization techniques. 
This involves introducing additional information or constraints to stabilize the solution and make the problem well-posed. Regularization can be performed by the substitution of $y$ with some more treatable operator, or in the case of Variational Regularization by the formulation of a different minimization problem. 
Often, a regularization approach can be understood both from an operator approximation and a variational point of view: this applies to Tychonoff regularization, which can be seen as the addition of a stabilizing term to Problem~\ref{eq:IP1}
\begin{equation}\label{eq:Tycho}
    \min_{p\in \Theta} \big\| y(p) - y^m \big\|_\mc Y^2 + \lambda\big\| p - p^0 \big\|_\Theta^2,
\end{equation}
for some norm $\|\cdot\|_\Theta$ on $\Theta$, $p^0$ in $\Theta$ and $\lambda $ in $ \R^+$. \medskip

As we will see in the next section, both these techniques can be understood by adopting a statistical perspective to IPs, which considers observations corrupted by noise with random behavior and known distribution.
A parameter-to-observation relation is assumed, rendering the available observation $y^m$ a realization of a random variable $Y^m$ over the measurement space $\mc Y$.
We consider an additive noise model
\begin{equation}\label{eq:par-to-obs}
    Y^m = y(p) + N,
\end{equation}
for noise $N$ with distribution $\bb P _N$. \newline
Equation~\eqref{eq:par-to-obs} deals that for any $p$ in $\Theta$, the distribution of $Y^m$ given $p$, noted $\bb P_{Y^m \mid p}$, is given by 
\[
    \bb P_{Y^m \mid p} (A) = \bb P_N (A-y(p))
\]
for any Borel set $A$ in $\mc B (\mc Y)$. 
Note that as $p$ is not a random variable, the distribution $\bb P_{Y^m \mid p}$ is not a conditional distribution and in this context the notation is meant to highlight the dependence of the distribution on the parameter $p$. \newline
For $\mc Y$ of finite dimension, the above equation can be expressed in terms of the probability density functions of $N$ and $Y^m$ with respect to the Lebesgue measure $m_\mc Y$.
As a function of the parameter $p$, the density of $Y^m$ given $p$ is called likelihood and given by
\begin{equation}\label{eq:likelihood}
    L(p) = \pi_{Y^m \mid p} (y^m) = \pi_N(y^m - y(p)).
\end{equation}
The likelihood of a problem is a crucial element in statistical IPs. \medskip

A natural development of the statistical approach to Inverse Problems are Bayesian Inverse Problems (BIPs). 
In the Bayesian setting, the unknown solution $p*$ of the IP is treated as a random variable $P$ and the solution of the IP then becomes the conditional distribution $\bb P_{P \mid Y^m=y^m} $ of $P$ given $Y^m=y^m$.

In full generality, the following result guarantees the possibility of formulating BIPs for arbitrary dimensionality:
\begin{thm} [Regular conditional probability]
    Let $ (\Omega, \mc F , \bb P) $ be a probability space, $\Theta$ be a separable Banach space equipped with the Borel $\sigma$-algebra $\mc B (\Theta)$, $(\mc Y, \tilde{\mc F})$ be a measurable space.
    Further, let $Y^m:\Omega \rightarrow \mc Y$ and $P : \Omega \rightarrow \Theta$ be random variables with $P \in L^1(\Omega, \bb P; \Theta) $. \newline
    Then there exists a $\bb P_{Y^m}$-a.s. unique map $\bb P_{P \mid Y^m} : \mc B (\Theta) \times \mc Y \rightarrow [0,1] $ such that :
    \begin{itemize}
        \item $\bb P_{P \mid Y^m}(\cdot, y)$ is a probability density on $\Theta$ for all $y$ in $\mc Y$;
        \item $\bb P_{P \mid Y^m}(A, \cdot)$ is measurable for all $A$ in $\mc B (\Theta)$;
        \item for all $B$ in $\sigma(Y^m)$, $A$ in $\mc B (\Theta)$, it holds that
                \[ 
                \int_B \bb P_{P \mid Y^m}(A, Y^m(\omega)) \ d\bb P(\omega)= \int_B \ind_A(P(\omega)) \ d\bb P(\omega).
                \] 
    \end{itemize}
    Such map is known as the regular conditional probability of $P$ given $Y^m$ and any map satisfying the first two properties is called a Markov kernel.
\end{thm}

This results guarantees the well-definiteness of conditional probabilities in a general setting, thus allowing for the formulation of BIPs in arbitrary dimensionality, but does not provide a direct way to formulate the posterior distribution given a parameter-to-observation relation. 
This is provided by Bayes' rule, which in generality is given by the following result from~\cite[Theorem 14]{DashtiStuart2017}:

\begin{thm}[Bayes' rule]
    Let $ (\Omega, \mc F , \bb P) $ be a probability space and $\Theta, \mc Y$ be separable Banach spaces equipped with the respective Borel $\sigma$-algebras. 
    Moreover, let $P : \Omega \rightarrow \Theta$ and $N:\Omega \rightarrow \mc Y$ be independent random variables, and $ Y^m = y(P) + N$ with $y: \Theta \rightarrow \mc Y$ a measurable map. \newline
    Assume $\bb P_{Y^m\mid P}(\cdot, p) \ll P_N$ for every $p \in \Theta$, that $\frac{d \bb P_{Y^m\mid P}(\cdot, p)}{d\bb P_N}(y) $ is $\bb P_{(P,N)}$-measurable, and that 
    \[
        \int_\Theta \frac{d \bb P_{Y^m\mid P}(\cdot, p)}{d\bb P_N}(y) \ d\bb P_P(p) > 0 \ \text{ for }  \ y, \ \bb P_N \text{-a.s.}
    \]
    holds.
    Then, the regular conditional probability $\bb P_{P\mid Y^m}(\cdot,y)$ for $P$ given $Y^m$ exists and is such that $\bb P_{P\mid Y^m}(\cdot, y) \ll \bb P _P$ $\bb P_{Y^m}$-a.s., with Radon-Nikodym derivative
    \begin{equation}\label{eq:infdimBayes}
        \frac{d\bb P_{P\mid Y^m}(\cdot, y)}{d\bb P_P}(p) = \frac{1}{Z(y)}\frac{d\bb P_{Y^m\mid P}(\cdot, p)}{d\bb P_{N}}(y).
    \end{equation}
\end{thm}

In the above theorem, $\bb P_P$ is the prior distribution of $P$ and $\bb P_{P \mid Y^m}(\cdot, y^m)$ is then the Bayesian posterior distribution of $P$ given $Y^m=y^m$.
As in~\cite[Theorem 6.31]{Stuart2010}, under certain hypothesis over the measurement space $\mc Y$, the distribution of the noise $\bb P_N$ and the forward model $y$, one has that the conditions over $\frac{d \bb P_{Y^m\mid P}(\cdot, p)}{d\bb P_N}(y)$ hold, rendering the application of the Theorem possible. \medskip

As for the scope of this work it is not necessary to work in full generality, now and for the rest of this work on it will be assumed that the involved spaces $\Theta, \  \mc Y$ are finite dimensional Banach spaces, and that $P$ and $Y$ are random vectors that admit a joint probability density function $\pi_{P,Y}$ with respect to the Lebesgue product measure $m_\Theta \otimes m_\mc Y$. 
This allows for a more intuitive and direct formulation of the Bayesian posterior distribution of $P$ given $Y=y^m$ by exploiting the conditional probability density.

\begin{thm}[Bayes' rule with Lebesgue measure]
    Let $Y = y(P) + N$ hold, with $P$ and $N$ independent and $y: \Theta \rightarrow \mc Y$ a measurable map.
    Then the conditional density of $Y^m$ given $P$ coincides with the likelihood~\eqref{eq:likelihood}, \[
        \pi_{Y^m\mid P = p}(y^m) = \pi_{N}(y^m - y( p) ) = L(p),
    \] and the posterior distribution of $P$ given $Y^m=y^m$ is given by the probability density \begin{equation}\label{eq:Bayes}
        \pi_{P\mid Y^m = y^m}(p) = \frac{\pi_{N}(y^m - y( p) ) \pi_P(p)}{\pi_{Y^m}(y^m)},
    \end{equation}
    where $\pi_{Y^m}(y^m)$ is the marginal density of $Y^m$ given by
    \[
        \pi_{Y^m}(y^m) = \int_\Theta  \pi_{N}(y^m - y( p) ) \pi_P(p) \ dm_\Theta(p).
    \]
\end{thm}

\subsection{Solutions of Inverse Problems}\label{sec:IP-sol}

What it means to solve an IP depends on its formulation and on the objectives that the solution serves.   
The least-squares formulation~\eqref{eq:IP1}, the regularization approach~\eqref{eq:Tycho} and non-Bayesian statistical IPs usually rely on point estimates; the Bayesian formulation instead can also provide a posterior distribution of the unknown given the observations. \medskip

In a statistical framework, the likelihood~\eqref{eq:likelihood} can be utilized to obtain a point estimate of the unknown $p*$ by Maximum Likeihood (ML) estimation
\begin{equation}\label{eq:ML}
    p^* = \arg \max_{p \in \Theta} L(p),
\end{equation}
which maximizes the likelihood of the observations $y^m$ given the parameter $p$.
As the likelihood often decays rapidly it is common to minimize the negative log-likelihood instead, solving the mathematically equivalent but numerically more stable problem \[
    p^* = \arg \min_{p \in \Theta} -\log L(p).
\]
For normal centered noise $N \sim \mc N(0, \Sigma^2)$, the negative log-likelihood is given by
\[
    -\log L(p) = \frac{1}{2} \| y^m - y(p) \|_{\Sigma^{-2}}^2 + \text{const}
\]
where $\| \cdot \|_{\Sigma^{-2}}$ is the norm induced by the inverse of the covariance matrix $\Sigma^2$ of the noise $N$ and $\text{const}$ is a constant term independent of $p$. 
By inserting the above expression in the minimization problem, we see that the ML estimate is equivalent to the minimization of the least-squares problem~\eqref{eq:IP1} with the choice of the norm $\| \cdot \|_{\Sigma^{-2}}$ on $\mc Y$.
By this, the statistical approach naturally provides both a justification and an interpretation for least-squares in IPs; the same happens to regularization techniques, but to see that we need to consider the Bayesian approach. \newline
Within the Bayesian framework, a point estimate can be obtained by Maximum A Posteriori (MAP) estimation which maximizes the posterior density~\eqref{eq:Bayes} of the unknown $P$ given the observations $Y^m=y^m$
\[
    p^* = \arg \max_{p \in \Theta} \pi_{P\mid Y^m = y^m}(p) = \arg \max_{p \in \Theta} L(p) \pi_P(p),
\]
where in the second equality we omitted the denominator $\pi_{Y^m}(y^m)$ as it is independent of $p$.
For normal centered noise $N \sim \mc N(0, \Sigma^2)$ and normal prior $P \sim \mc N(p_0, \Sigma_p^2)$, if we take the negative logarithm of $\pi_{P\mid Y^m = y^m}(p)$ and omit the constants, we can rewrite the MAP problem as
\[
    p^* = \arg \min_{p \in \Theta} \|y^m - y(p) \|_{\Sigma^{-2}}^2 + \|p - p_0 \|_{\Sigma_p^{-2}}^2.
\] 
This, for $\|\cdot \|_\mc Y = \|\cdot \|_{\Sigma^{-2}}$ and $\|\cdot \|_\Theta = \|\cdot \|_{ \lambda \Sigma_p^{-2}}$ is exactly the Least-Squares problem with Tychonoff regularization~\eqref{eq:Tycho}, allowing us to interpret the regularization term as an expression of prior beliefs over the parameter $p$ and the parameter $\lambda$ as a measure of confidence in the prior.
\medskip


While the point estimates can be significantly cheaper to compute, they do not provide a full characterization of the uncertainty in the solution by themselves.
To obtain a representation of the posterior distribution~\eqref{eq:Bayes} of the parameter $P$ given the observations $Y^m=y^m$, we consider Markov Chain Monte Carlo (MCMC) sampling.
MCMC techniques usually require a greater number of evaluations of the likelihood function and consequently of the forward model, but then provide a set of correlated samples from $\pi_{P\mid Y^m = y^m}$. 
Such a set of samples is particularly useful as it allows for Monte Carlo integration
\begin{equation}\label{eq:MC-integration}
    \bb E _{X \sim \bb P_X} [f(X)] = \int_\mc X f(x) \pi_X(x) \ dx \approx \frac{1}{N} \sum_{i=1}^{N} f(x_i).
\end{equation}
Under certain hypothesis on the generation of the samples $\{x_i\}_{i=1}^N$ from $\bb P_X$, a corresponding version of the Law of Large Numbers can guarantee convergence of the right-hand side to the left-hand side for $N \rightarrow \infty$ for any measurable function $f$ integrable under $\bb P_X$.
For $X = P \mid Y^m = y^m$, Monte Carlo integration allows for the computation of several quantities of interest, such as for example the posterior mean and variance.

The first MCMC technique to be introduced was the Metropolis-Hastings (MH) algorithm, first introduced by~\cite{MetropolisRosenbluthRosenbluthTellerTeller1953} and later generalized by~\cite{Hastings1970}.
As MH includes key concepts that are also present in other MCMC techniques while being fairly simple, we will briefly introduce it here.
MH consists of the following algorithm:

\par\noindent\rule[1mm]{\textwidth}{0.4pt}
\phantomsection \makeatletter\def\@currentlabel{Algorithm }\makeatother\label{algo:MH}
\large{\textbf{Algorithm 1 - Metropolis-Hastings.} } \normalsize
\par\noindent\rule[2mm]{\textwidth}{0.2pt}
\textbf{Require:} $\bb P$ target distribution with density $\pi(p)$, a Markov kernel $\mc K$ with density $q(p \mid p ' )$, initial state $p_0$, $N$ samples to draw.
\par\noindent\rule[2mm]{\textwidth}{0.2pt}
For $i =1, \dots, N$ do: 
\begin{enumerate}
    \item Draw proposal $p_n$ from $\mc K (\cdot, p_{n-1})$
    \item Set acceptance threshold: \[ \alpha = \frac{\pi(p_n) q(p_{n-1} \mid p_n )}{\pi(p_{n-1}) q(p_n \mid p_{n-1} )} \]
    \item Draw $u$ from $\mathcal{U}([0,1])$
    \item If $u > \alpha$ then reject: $p_n \leftarrow p_{n-1}$
\end{enumerate}
\textbf{Return} $p_1, \dots, p_N$.
\par\noindent\rule[3.5mm]{\textwidth}{0.4pt}

We observe that, as the target distribution's density is utilized to compute the acceptance threshold $\alpha$ only through a ratio, it needs to be known up to a normalizing constant: this is crucial for the application in BIPs, where the normalizing constant is given by the marginal density $\pi_{Y^m}(y^m)$ which is usually unknown and whose estimation is usually challenging.
This property is fortunately shared by all MCMC techniques \newline
Similarly to Importance Sampling, MH relies on a proposal distribution $\mc K(\cdot, p)$ to generate samples; the rejection step then guarantees that the target distribution is invariant with respect to the sample chain $\{p_i\}$.
The proposal kernel $\mc K$ is crucial both for guaranteeing the convergence of the chain to the target distribution and for the efficiency of the sampling process. 
A sufficient condition for convergence is the absolute continuity of the proposal with respect to the target, i.e. $\mc K(\cdot, p) \ll \bb P $ for every $p$; however, a proposal tailored to the target will result in a better acceptance rate for new samples and both the convergence velocity and the quality of the samples will be higher. 

A number of techniques has been developed to address the issues related to the quality of the sampling process.
First, instead of fixing the proposal distribution, one can choose a family of kernels $\mc K_\theta$ and then adjust the parameter $\theta$ during the sampling process: for random walk MH, the optimal acceptance ratio balancing the exploration of the parameter space and the acceptance of new samples is around $0.234$~\cite{GelmanGilksRoberts1997}, and the proposal distribution can be tuned to achieve this value.
Second, a burn-in or warm-up period can be introduced in order to avoid transient behavior by allowing the chain to converge to the target distribution before starting to collect samples; this reduces the dependence of the resulting chain on the initial state and can improve the quality of the resulting chain.
Third, convergence diagnostics can be performed to assess the state of the chain's convergence to the target distribution and the mixing of the samples.
A commonly employed diagnostic is the Gelman-Rubin potential scale reduction $\hat R$ introduced by~\cite{GelmanRubin1992}, which utilizes multiple independent chains by computing the variance within each chain and the variance between all chains.
Let $\{ \theta_{i,j} \}_{j=1}^{N} \subseteq \R $ be independent chains of length $N$ with $i =1, \dots, M$, and let the mean of each chain and overall mean be 
\[
    \bar \theta_i = \frac{1}{N} \sum_{j=1}^{N} \theta_{i,j}, \quad \text{and} \quad \bar \theta = \frac{1}{M} \sum_{i=1}^{M} \bar \theta_i.
\]
Then, the between chain variance is given by
\[
    B = \frac{N}{M-1} \sum_{i=1}^{M} (\bar \theta_i - \bar \theta)^2,
\]
while the within chain variance is given by
\[
    W = \frac{1}{M} \sum_{i=1}^{M} \frac{1}{N-1} \sum_{j=1}^{N} (\theta_{i,j} - \bar \theta_i)^2.
\]
An unbiased estimator of the variance of the target distribution is then given by
\[
    \hat \sigma^2 = \frac{N-1}{N} W + \frac{1}{N}B,
\]
and the potential scale reduction is defined as 
\[
    \hat R = \sqrt{ \frac{\hat \sigma^2}{W} }.
\]  
$\hat R$ quantifies the difference in variance between the different chains and is expected to converge to $1$ from above as the chains converge to the stationary distribution. 
Two common practices, both recommended by~\cite[Chapter 11.4]{GelmanCarlinSternDunsonVehtariRubin2013}, are to consider $\hat R < 1.1$ as a convergence criterion and to split each chain into two halves before computing the potential scale reduction in order to test for adequate mixing of each chain.
For multivariate distributions while generalizations of the potential scale reduction $\hat R$ exist, such as the one proposed by~\cite{BrooksGelman1998}, in practice it is common to consider the potential scale reduction of each component of the parameter vector separately and then to take the maximum value of the potential scale reduction across all components.

Once convergence to the stationary distribution is achieved, the generated samples will be correlated and the effects of autocorrelation need to be taken into account when performing Monte Carlo integration; in fact, the variance of the Monte Carlo estimator~\eqref{eq:MC-integration} will be increased by the presence of autocorrelation as compared to i.i.d. data.
For samples $\{ \theta_j\}_{j=1}^N$ from a Markov chain $X = (X_1,X_2, \dots, )$ the correlation at lag $k$ is defined as
\[
    \rho_k = \frac{\text{Cov}[X_1, X_{k+1}]}{\text{Var}[X_1]},
\]
and the chain's autocorrelation time as
\[
    \tau_X = 1 + 2 \sum_{k=1}^{\infty} \rho_k.
\]
Under the hypothesis that $X$ admits a stationary distribution $\pi$ and that the chain is initialized in the stationary state, i.e. $X_1 \sim \pi$, the finite-dimensional version of the Central Limit Theorem proved by~\cite{KipnisVaradhan1986} guarantees that 
\[
    \sqrt{\frac{\tau_X}{N}} \left(  \bb E_\pi [X_1] - \frac{1}{N} \sum_{j=1}^{N} X_j \right) \xrightarrow{d} \mc N(0, \text{Var}[X_1]).
\]
In particular this implies that the variance of the mean estimator $\frac{1}{N} \sum_{j=1}^{N} X_j$ is not given by $\frac{1}{N} \text{Var}[X_1]$ as in the i.i.d. case, but by $\frac{\tau_X}{N} \text{Var}[X_1]$.
This motivates the introduction of the effective sample size (ESS) for the samples $\{\theta_j\}$ of the chain $X$ 
\begin{equation}
    \text{ESS}_{\{\theta_j\}} = \frac{N}{\tau_X} = \frac{N}{1 + 2 \sum_{k=1}^{\infty} \rho_k} \leq N
\end{equation}
where equality holds if and only if the samples are uncorrelated.
The ESS can be understood as the number of independent samples that would provide the same variance as the correlated samples $\{\theta_j\}$, and can be used to assess the quality of the samples generated by the chain.
The larger the autocorrelation time $\tau_X$ the smaller the ESS, requiring more samples achieve a desired certainty level.
In practice, the ESS can be estimated by computing the autocorrelation of the samples $\{\theta_j\}$ and then truncating the sum at a certain lag $k$; this is often used in practice to estimate the ESS~\cite{Geyer1992}.
If the autocorrelation time is significant, Monte Carlo integration can be rendered more computationally efficient by thinning the chain and keeping only every $k$-th sample, i.e. to consider the samples $\{\theta_{k \cdot h}\}_{h=1}^{N/k}$ instead of $\{\theta_j\}_{j=1}^N$.\medskip

Among MCMC techniques, we mention Ensemble Sampling, whose implementation we will use in the numerical experiments of Section~\ref{sec:exp}.
Ensemble Sampling is a MCMC technique introduced by~\cite{GoodmanWeare} which guarantees invariancy with respect to affine transformation of the parameter space by utilizing multiple correlated chains. 
The effectiveness of sampling crucially depends on the proposal function, which in the case of Ensemble samplers are also known as moves.
As the posterior distribution could potentially exhibit multimodal behavior, we adopt the Differential-Independence Mixture Ensemble (DIME) move introduced by~\cite{Boehl}, which adapts to Ensemble sampling the Differential Evolution MCMC framework introduced by~\cite{TerBraak} and behaves efficiently on multimodal distributions. 

\subsection{Partial differential equation models} \label{sec:PDE}

\subsection{Adaptive Finite Element method} \label{sec:AdaFE}