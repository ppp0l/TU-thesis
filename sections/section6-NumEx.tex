\section{Numerical experiments}\label{sec:exp}

In this section, we present numerical experiments to demonstrate the performance of~\ref{algo:AL} under different conditions. 
Among the three presented experiment, the first two use an analytical solution of the underlying PDE, while the third one is based on actual FE simulations of a PDE.
The choice of analytical forward models allows to evaluate the actual error and compare convergence rates and reduces the time required by realizations of~\ref{algo:AL}. \medskip 

The remainder of this section is organized as follows: in Section~\ref{sec:setup}, we describe the objectives of the experiments and general setup across them; Section~\ref{sec:implementation} provides implementation details; in Section~\ref{sec:3dexp}, we present an analytical example with 3D parameter space based on the Laplace equation; in Section~\ref{sec:6dexp}, we present an analytical example with 6D parameter space based on the diffusion equation; in Section~\ref{sec:FEexp}, we present a FE example based on elastomechanics with 2D parameter space; finally, in Section~\ref{sec:concl}, we summarize the conclusions drawn from the experiments.

\subsection{Objectives and general setup}\label{sec:setup}

The experiments aim at testing the proposed strategy on low to moderate-dimensional parameter spaces.
The scope of these examples can be articulated in three senses:
\begin{enumerate}
    \item comparing the training strategy as compared to other strategies under the same computational budget and for the same surrogate model;
    \item comparing the performance of LR surrogates to the more established GPR surrogate models across different training strategies;
    \item comparing the effect on GPR of discretization noise estimation as given by Equation~\eqref{eq:shrinkage-estimator} in Section~\ref{sec:GPAL}.
\end{enumerate}
While the first and the last points can be evaluated more easily, the second point is more difficult to evaluate as it requires a fair comparison between the two kinds of surrogate models.
Consequentely for point 2, we will exploit the available ground-truth in the analytical examples, while for the FE example we will resort to a qualitative comparison. \medskip

As a benchmark, we consider the non-adaptive space-filling approach given by Latin Hypercube Sampling (LHS)~\cite{McKayBeckmanConover1979}, the approach proposed in~\cite{Dinkel2024}, which also relies on interleaved sampling but selects the candidates randomly from the current posterior approximation, both using fixed tolerances.
Moreover, we also consider the fixed tolerance version of the proposed strategy, which is given by~\ref{algo:AL} without the tolerance optimization step. \medskip

Unlike the experiments in~\cite{VillaniArconesUngerWeiser2025}, where various setups for the budget fractionating and the FE cost depending on the fraction $\frac{l}{r}$ were considered, in this work we fix them and do not investigate their effect on the performance of the algorithm.
Previous works~\cite{SemlerWeiser2023,VillaniArconesUngerWeiser2025,VillaniUngerWeiser2024} have shown that the effect of an higher FE cost is to reduce the effectiveness of tolerance optimization, resulting in performances similar to those of a fixed tolerance strategy.
We consider uniform budget fractionating, spending the same amount of computational resources for Problem~\eqref{prob:incremental-doe} at each iteration, and we set the fraction $\frac{l}{r}$ to $1$ for the analytical examples and to $1.5$, corresponding to quadratic FE $r=2$ on a 3D mesh $l=3$, for the elastomechanics example. \medskip

In all experiments, the measured quantity is directly obtained from pointwise evaluations $f(\theta)$ of the equation's solution $f$, for $\theta$ in a set of sensors $\mc S$; consequently, the measurement operator $H$ is given by
\[
    H(f(\cdot; p)) = \left( f(\theta_j)\right)_{j=1,\ldots, \text{dim} \mc Y} \ \text{ for some ordering } \ \theta_1, \ldots, \theta_{|\mc S|} \ \text{ of the sensors } \ \mc S.
\]
In the analytical examples $f(p) \in \R$, while for the FE example $f(p)\in \R^3$: in the latter case, only the $z$-component of the displacement is measured. \medskip

For the analytical examples the analytical formula of a solution is available and no discretization is necessary.
This allows us to evaluate the forward model $y(p)$ with close to zero cost, rendering it possible to represent the ground-truth posterior and to perform repeated runs of the algorithm. 
On the downside, this requires us to simulate the discretization error; we do so by adding by adding pseudorandom noise $\nu$ with the appropriate distribution to forward model evaluations $y(p)$. \newline
When training GPR surrogates, we consider zero mean Gaussian noise $\nu \sim \mc N (0, \tau^2 \Sigma_T^2)$ when simulating $y_\tau(p)$, where \[
\big(\Sigma_T\big)_{ij} = \sigma\big( \|\theta_i - \theta_j\|_{\mc Y} \big) \ \text{ for } \ \theta_i, \theta_j \in \mc S
\]
with $\sigma$ a decreasing function such that $\sigma(0)=1$.
Such a setup allows us to evaluate the effect of the discretization noise on the GPR surrogate model.\newline
When training LR surrogates, we consider uniform noise $\nu \sim \mc U([-\tau, \tau]^{\text{dim} \mc Y})$ so that $I_\tau$ is an interval and the assumptions of propositions~\ref{prp:LR-PI},~\ref{prp:LR-likelihood} and~\ref{prp:EER} are met. \medskip 

For the analytical examples, we will perform and average repeated runs of the algorithm with the same budget but different random seeds, in order to compensate for the lack of discretization noise and to evaluate the sensitivity of the algorithms to randomness.
Details about the number of runs will be given in the respective sections. 

\subsection{Implementation details}\label{sec:implementation}
A Python implementation of~\ref{algo:AL}, available at (...) along with the results, has been developed to conduct the experiments on an Intel Core i7-9700T Ã— 8 CPU. \medskip

The Gaussian process surrogate model uses a GPyTorch~\cite{GPyTorchPaper} base model, set to work with double-precision floating-point numbers.
We consider a separable RBF kernel 
\[
    k_{(\mathbf{L}, \lambda)}(p, p') =  \exp ( - \norm{p - p'}^2_{L} ) \ K_\lambda,
\]
as introduced by equations~\eqref{eq:separable-kernel} and~\eqref{eq:RBF-kernel}, with $\lambda = (\mathbf{s}, \mathbf{C}) $ and
\[
K_\lambda = \text{diag}(\mathbf{s}) + \mathbf{CC}^T,
\] 
where $\mathbf{s} \in \R^{\text{dim} \mc Y}$ is a scaling vector and $\mathbf{C} \in \R^{\text{dim} \mc Y \times 2}$ is a rank 2 matrix which models the correlation among components.
GPyTorch's Adam is employed to optimize the hyperparameters on the marginal likelihood~\eqref{eq:marginal-likelihood}. \medskip

The two optimization problems to determine new candidate points and evaluation tolerances are solved through the SciPy optimizer \texttt{scipy.optimize.minimize}. 
For the GPR acquisition function, the exact gradient is not available and local maxima of~\eqref{eq:acq-fun-disc} are found using the L-BFGS-B algorithm~\cite{ZhuBirdNocedal}. 
As derivative information is available for the LR acquisition function~\eqref{eq:alpha-LR} and for the target of the tolerance problem, we employ the Sequential Least Squares Quadratic Programming (SLSQP) algorithm to solve Problem~\eqref{prob:discrete-tol} and to find maximizers of~\eqref{eq:acq-fun-disc} for a LR model. 
For tolerance optimization, before applying the SLSQP algorithm a coordinate transformation is applied in order to linearize the work constraint. 
In all cases, we adopt a multi-start approach by selecting a number of initial points through a low-discrepancy sequence. \medskip

To sample the posterior, we consider the \texttt{emcee}~\cite{emceePaper} implementation of Ensemble Sampling with the DIME move~\cite{Boehl} as introduced in Section~\ref{sec:IP-sol}. 
The posterior plots are then realized using the \texttt{corner} package~\cite{corner}. \medskip

While the interface for the experiments is also realized in Python, the FE model is implemented in C++17 through the \texttt{Kaskade 7} Finite Element toolbox~\cite{GoetschelSchielaWeiser2021}.\medskip

The rest of the implementation has been developed by the author and relies in particular on the \texttt{numpy} package~\cite{numpy}, while the \texttt{matplotlib} package~\cite{matplotlib} has been employed for the generation of plots. 



\subsection{3D analytical example - Laplace equation}\label{sec:3dexp}

The first numerical experiment involves the distributional Laplace equation on $\R^3$
\[
 \Delta f = \delta_{x_0}
\] 
with a Dirac's delta source centered in some $x_0 \in \R^3$ and decaying to zero at infinity
\[
\lim_{\| x \| \to \infty} f(x; x_0) = 0.
\]
As a solution, we consider Green's function for the 3D Laplace equation
\[
f(x; x_0) =  \frac{1}{2 \|x-x_0\|_2},
\]
where a multiplicative constant has been added for numerical stability reasons. \medskip

To formulate an IP, $x_0$ is assumed to be an unknown parameter and we aim at identifying it by measurements of $f(x;x_0)$ in sensors $x \in \mc S$ for 
\[
    \mc S = \left\{
        \begin{pmatrix}
            \cos( i \frac{\pi}{3}) \sin( j \frac{\pi}{2}) \\
            \sin( i \frac{\pi}{3}) \sin( j \frac{\pi}{2}) \\ 
            \cos( j \frac{\pi}{2})
        \end{pmatrix} 
        \in \R^3 \ \Big| \ i \in \{0,1,2,3,4,5\}, \ j \in \{0,1,2\}
     \right\},
\]
resulting in 14 sensors on the unit sphere and consequently a 14-dimensional measurement space $\mc Y = \R^{14}$. \newline
The parameter space is assumed to be $\Omega = \left[-\frac{1}{2}, \frac{1}{2}\right]^3$ and a Normal prior $\mc N (0, \frac{1}{6}I_3)$ is considered. \medskip

We generate measurements by adding pseudorandom noise $N \sim \mc N (0, \sigma^2 I_{14})$, with $\sigma = 10^{-2}$ to the exact forward model evaluations $y(p)$ for some $p$. 
We consider 5 different measurements corresponding to 5 different $p$ randomly extracted from the prior distribution and, as the discretization error is simulated, we perform 5 runs of each training strategy with different random seeds for each measurement.
\medskip

\subsection{6D analytical example - Diffusion equation}\label{sec:6dexp}

The second numerical experiment involves the diffusion equation on $\R^3$
\[
f_t = \Delta f
\]
with initial conditions given by two Dirac's delta of opposite sign centered in some $x_p$ and $x_n$:
\[
f (0,x; x_p, x_n) = \delta_{x_p}(x) - \delta_{x_n}(x).
\]
We consider the difference of fundamental solutions of the diffusion equation centered in $x_p$ and $x_n$ as a solution of the above problem
\[
f(t,x; x_p, x_n) = (4 \pi t)^{-\frac{3}{2}} \left( \exp \left( - \frac{\norm{x-x_p}_2^2}{4t}\right)- \exp \left( - \frac{\norm{x-x_n}_2^2}{4t}\right) \right).
\]
We treat $x_p, x_n$ as unknown parameters and want to identify them by measuring $u(t,x;x_p, x_n)$ for certain $t,x$. As a parameter space, $\Omega = [-1,1]^6$ is considered and a Normal prior $\mc N (0, \frac{1}{4}I_6)$.

\subsection{2D Finite Element example - Elastomechanics}\label{sec:FEexp}



\subsection{Conclusions}\label{sec:concl}
