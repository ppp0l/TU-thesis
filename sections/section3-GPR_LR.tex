\section{Stochastic surrogate models}\label{sec:surrogates}
To reduce the computational costs related to the evaluation of the forward model, a surrogate model is an attractive option.
The scope of the surrogate model is to approximate the forward model $y : \Theta \rightarrow \mc Y $ or the problem's likelihood $L: \Theta \rightarrow \R $ directly with a simpler and faster model.\medbreak

Whether the surrogate aims at representing the forward model or the likelihood directly is a matter of convenience.
Producing a surrogate representation of the forward model guarantees a cheaper representation of the likelihood, but not vice versa. Moreover, the values the likelihood assumes are constrained to be positive, which can be non-trivial to impose for certain classes of surrogates; however, an argument in favor of adopting a surrogate for the likelihood is that, if the forward model has a great number of output components, the computational costs of a likelihood surrogate can be significantly lower than the ones of a forward model surrogate.

Ultimately, the choice of the target of the surrogate model depends on the adopted surrogating technique, the usage of the surrogate and the objective being pursued.
When a point estimate of the parameter is a sufficient solution for the IP, adopting a Bayesian Optimization framework and surrogating the likelihood directly is the most natural choice.
Instead, when aiming for a posterior representation through sampling, approaches such as Delayed Acceptance MCMC \cite{ChristenFox2005} are indifferent to the choice.
For the approach presented in this work, a forward model representation will be obtained, as we will exploit the error estimators for FE models in the surrogate's training. \medbreak

We will consider training data $ D =\{ (p_i, \tau_i, y_i) \}_{i=1}^n $, where $ p_i \in \Theta $ are the input parameters, $ \tau_i \in \R_+ $ are the tolerances of the model's evaluation, and $ y_i \in \mc Y $ are the output values of the numerical forward model,\[ y_i = H(u_{\tau_i}(\cdot; p_i)). \]

There exist different classes of surrogate models: an useful distinction is the one between deterministic and stochastic surrogate models.
Deterministic surrogate models are based on deterministic functions, such as polynomial regression, neural networks, or support vector machines.
They provide an approximation of the forward model or the likelihood, but they do not provide by default an estimate of their own uncertainty.
On the other hand, stochastic surrogate models are based on probabilistic assumptions and naturally provide an estimate of the uncertainty associated with their prediction.
This is particularly useful in the context of BIPs, as the uncertainty of the surrogate model can be incorporated in to the problem's likelihood, as illustrated in Section~\ref{sec:likelihoods}. \medbreak

We will adopt stochastic surrogate models, which for any $p \in \Theta$ provide a random prediction with a certain predictive distribution. 
The surrogate model could thus be viewed as a stochastic process, but as this is not necessary for the scopes of this work, we see it as a function that to each parameter $p$ associates a probability distribution $ P( y(p) \mid D)$ in the set $\mc P (\mc Y)$ of probability distributions over $\mc Y$.
For training data $D$, we write \begin{align*}
    y_D : \Theta \rightarrow \mc P( \mc Y) \\
    p \mapsto \bb P( y(p) \mid D).
\end{align*}

In the next two sections, we introduce two techniques which provide a stochastic surrogate model: Gaussian process regression (GPR) and Lipschitz regression (LR).

\subsection{Gaussian process regression}\label{sec:GPR}
Gaussian process regression (GPR) is a non-parametric regression technique which relies on probabilistic assumptions and is closely related to support vector machines and kernel methods.

The core idea is that the underlying function $y$ is a realization of a Gaussian process and, assuming the observation are contaminated by Gaussian noise, GPR obtains a closed-form prediction through Gaussian conditioning.\medbreak

\begin{dfn} [Gaussian Process]
    A Gaussian process (GP) over $\mc Y$ is a collection $Y = \{Y_p\}_{p\in\Theta}$ of $\mc Y$-valued random variables, such that for any finite subset $p_1, \ldots, p_n \in \Theta$, the random variables $Y_{p_1}, \ldots, Y_{p_n}$ have a joint Gaussian distribution.

    A Gaussian process is completely defined by its mean function $m: \Theta \rightarrow \mc Y$ and its covariance function $k: \Theta \times \Theta \rightarrow SPSD_{\text{dim}\mc Y}(\R)$, also known as kernel. 
    We denote $Y \sim GP(m, k)$ for a Gaussian process of mean $m$ and kernel $k$.
\end{dfn}
Note that the covariance operator $k(p, p')$ can be identified with a positive semi-definite matrix as $\mc Y$ is of finite dimension. \medbreak

Gaussian process regression relies on two assumptions:
\begin{itemize}[font=\itshape, leftmargin=1.5cm, align=right, labelwidth=2.4cm]
    \item[(GPR-model)] the forward model $y$ is a realization of a GP $Y \sim GP(m, k)$;
    \item[(GPR-data)] the observations are contaminated by Gaussian noise independent from $Y$, i.e. 
    \[ 
        y_i = y(p_i) + \nu_{D,i} \ \text{ for every } \ (p_i,\tau_i,y_i) \ \text{ in } \ D 
    \] 
    holds, where $\nu_{D,i} \sim \mc N(0, \Sigma_D(\tau_i)^2)$ for some $\Sigma_D(\tau)$, and $\nu_{D,i}$ is independent from $Y$.
\end{itemize}  
The first assumption implicitly prescribes a certain regularity of the forward model depending on the choice of the kernel $k$, but does not impose any parametric form on the model itself. 
The choice of a kernel is crucial for the effectiveness of GPR: in order to allow for flexibility, in practice a parametric family of kernels $\{ k_\lambda\}$ is chosen to model the covariance, with $\lambda$ known as the kernel's hyperparameters. More remarks about kernel structure and hyperparameters will be made at the end of this section.

The second assumption allows to derive a closed-form expression for the predictive distribution of the GP, and in our context entails that the discretization error on numerical evaluations of the forward model is Gaussian, i.e. 
\[
    H(u_{\tau}(\cdot; p)) - H(u(\cdot; p)) \sim \mc N(0, \Sigma_D(\tau)^2);
\] for a general GPR task, it is common to assume 
\begin{equation}\label{eq:iid-GPR-noise}
    \Sigma_D(\tau) = \tau I_{\text{dim} \mc Y}
\end{equation} 
and often $\tau$ is unknown and fitted together with the kernel's hyperparameters. \newline
In the context we are working in, Equation~\eqref{eq:iid-GPR-noise} implies i.i.d. components of the FE discretization error, which is not realistic: Section~\ref{sec:GPAL} presents an approach to model the discretization error covariance by exploiting an adaptive FE scheme. \medbreak

Given a prior GP $Y \sim GP(m, k)$ and the training data $D$, the posterior GP $Y \mid D$ is obtained by conditioning the prior on the observations. 
The analytical expression for the posterior GP is given by the following theorem.
Although this result is foundational for GPR, its complete proof is rarely explicitly stated:~\cite[Appendix A]{RasmussenWilliams2006} provides the necessary results for the full proof, which relies on some technical linear algebra lemmas.

\begin{restatable}[GP posterior]{theorem}{GPpost} \label{thm:GP-posterior}
    Under assumptions \textit{(GPR-model)} and \textit{(GPR-data)}, assume that the covariance between the noise terms $\nu_{D,1},\dots,\nu_{D,n}$ is given by some positive definite $\Sigma_D(\mc T)^2$ for $\mc T = (\tau_1, \ldots, \tau_n)$. \newline
    Then the posterior GP $Y \mid D$ is a GP over $\mc Y$ with mean function $m_D$ and kernel $k_D$, where
    \begin{equation} \label{eq:GP-predictive}
        \begin{gathered}
            m_D(p) = m(p) + k(p, P) (k(P,P) + \Sigma_D(\mc T)^2 )^{-1} (y - m(P))  \\
            k_D(p, p') = k(p, p') - k(p, P) (k(P,P) + \Sigma_D(\mc T)^2 )^{-1} k(P, p')^T            
        \end{gathered}
    \end{equation}
    for $P = (p_1, \ldots, p_n)$ and block matrices $K(P,P)_{i,j} = k(p_i, p_j)$, and $k(p, P) = (k(p, p_1), \ldots, k(p, p_n))$.
\end{restatable}

As we are not interested in the full GP posterior but only in the point-wise predictive distribution, the following corollary allows us to define the GPR surrogate we will employ.
\begin{cor}
    Under the assumptions of Theorem~\ref{thm:GP-posterior}, the GPR predictive distribution given the prior $GP(m,k)$ and the data $D$, is given by 
    \begin{gather*}
        y_{D, \text{GP}} : \Theta \rightarrow \mc P( \mc Y) \\
        p \mapsto \mc N( m_D(p), k_D(p,p) ),
    \end{gather*}
    with $m_D$ and $k_D$ are given by Equations~\eqref{eq:GP-predictive}.
\end{cor}

The core of GPR is now outlined, but for practical use a few more remarks are necessary. \newline
First, note that the covariance between the noise terms $\nu_{D,i}, \ \nu_{D,k}$ is assumed to be known in Theorem~\ref{thm:GP-posterior}, but in practice information about correlations is scarce or absent. Thus, usually a block diagonal structure is assumed 
\begin{equation} \label{eq:GPR-noise-corr}
    \Sigma_D(\mc T)^2 = \text{diag}(\Sigma_D(\tau_1)^2, \ldots, \Sigma_D(\tau_n)^2),
\end{equation}
entailing uncorrelated noise terms.

Moreover as mentioned above, the kernel structure and the treatment of hyperparameters play an essential role in GPR.

The choice of what parametric family of kernels $\{ k_\lambda\}$ to adopt determines the regularity of the GP's mean and realizations, see ~\cite{Kanagawa2018} and~\cite[chap. 4]{ChristmannSteinwart2008}; moreover, the parametrization $\lambda \mapsto k_\lambda$ needs to allow for efficient optimization of the hyperparameters $\lambda$. \newline
To ease the hyperparameters optimization problem and improve computational efficiency, the kernel $k_\lambda$ is often chosen to be stationary, i.e. 
\[
k_\lambda(p, p') = g_\lambda(p - p') \ \text{ for every } \ p, p' \in \Theta
\]
for some $g_\lambda:\Theta \rightarrow \R$.
For dim$\mc Y > 1$, it is common to model the correlation between inputs and outputs separately~\cite{AlvarezRosascoLawrence2012}; this results in what is known as a separable kernel 
\begin{equation}\label{eq:separable-kernel}
    k_\lambda(p, p') = \kappa_{\lambda_i}(p, p') K_{\lambda_o}
\end{equation}
for $\lambda = (\lambda_i, \lambda_o)$, with $\kappa_{\lambda_i}: \Omega \times \Omega \rightarrow \R$ and $K_{\lambda_o} \in R^{m\times m}$ being a scalar kernel and a symmetric positive definite matrix.
$\kappa_{\lambda_i}$ models the correlation between different inputs and $K_{\lambda_o}$ models the correlation between output components.

The possible choices for families of kernels are vast and it is important to note that sums and products of kernels are again kernels, allowing for the construction of complex kernel structures by the combination of elementary kernels~\cite{Duvenaud}.
Among such elementary kernels, particularly relevant are the Radial Basis Function (RBF) kernel and the Matérn kernel. \newline
The expression of the RBF kernel, also known as squared exponential kernel, is given by
\begin{equation}\label{eq:RBF-kernel}
    k_{\lambda}(p, p') = \exp\left(-\frac{\|p - p'\|_\Theta^2}{2\lambda^2}\right),
\end{equation}
with $\lambda$ being a length-scale parameter. 
Note that more complex RBF kernels can be obtained by altering the norm $\| \cdot \|_\Theta$, and for a parametrized norm this leads to what is known as Automatic Relevance Determination (ARD).
The RBF kernel leads to a $\mc C^\infty$ mean prediction and is usually adopted as a default choice whenever the underlying function can be assumed to be rather smooth. \newline
The Matérn kernel allows for less-regular predictions. Its general expression is given by
\[
k_{(\gamma, \lambda)}(p, p') = \frac{2^{1-\gamma}}{\Gamma(\gamma)} \left( \frac{\sqrt{2\gamma} \|p - p'\|}{\lambda} \right)^\gamma K_\gamma \left( \frac{\sqrt{2\gamma} \|p - p'\|}{\lambda} \right),
\]
where $\Gamma$ is the Gamma function $K_\gamma$ is a modified Bessel function~\cite[sec. 9.6]{AbramowitzStegun1964};
for $\gamma = n + \frac{1}{2}$ with $n\in \bb N$, the expression simplifies to 
\[
     k_{n+\frac{1}{2}, \lambda}(p, p')=\exp \left(-{\frac {{\sqrt {2n+1}}\|p - p'\|}{\lambda }}\right){\frac {n!}{(2n)!}}\sum _{i=0}^{n}{\frac {(n+i)!}{i!(n-i)!}}\left({\frac {2{\sqrt {2n+1}}\|p - p'\|}{\lambda }}\right)^{n-i}.
\]
The regularity of the predictive mean depends on the parameter $\gamma$: the mean prediction is $\gamma + \frac{1}{2}$-times weakly differentiable.
Most commonly, $\gamma$ is set to $\frac{3}{2}, \frac{5}{2}$ or $\frac{7}{2}$, as $\gamma = \frac{1}{2}$ leads to a non-differentiable mean prediction, and for $\gamma > \frac{7}{2}$ the high regularity favors the adoption of a RBF kernel.
Note that for $\gamma \rightarrow \infty$ the Matérn kernel converges to the RBF kernel. \medbreak

The treatment of hyperparameters constitutes an estimation problem.
For training data $D =\{ (p_i, \tau_i, y_i) \}_{i=1}^n$, the distribution of the data given the hyperparameters $\Lambda = \lambda$ is given by
\[
    (Y_{p_1}, \dots, Y_{p_n}) \mid \Lambda \sim \mc N \left( (m(p_1), \dots, m(p_n)), k_\Lambda(P,P) + \Sigma_D(\mc T)^2 \right)
\]
with $k(P,P)$ and $\Sigma_D(\mc T)$ as given in Theorem~\ref{thm:GP-posterior};
this results in the hyperparameters' likelihood given the data,
\begin{equation} \label{eq:marginal-likelihood}
    L(\lambda) = \pi_{(Y_{p_1}, \dots, Y_{p_n}) \mid \Lambda} \big ( (y_1,\dots,y_n) \mid \Lambda = \lambda \big ).    
\end{equation}
To solve the problem, one either adopts a point estimate, given usually by ML or MAP, or marginalizes the GPR prediction over the hyperparameter posterior distribution by sampling. 
While marginalizing propagates the uncertainty in the hyperparameters, it is computationally considerably more expensive: due to this reason a point estimate is most commonly employed and one talks about hyperparameter optimization. Gradient-based methods are utilized in the problem's solution. \medbreak


\subsection{Lipschitz regression}\label{sec:LR}
Lipschitz regression (LR) is a non-parametric regression technique which provides uniform confidence intervals and hard bounds for the values of the target function. \medbreak

Similarly to GPR, LR depends on one assumption about the model and on one assumption about the data; unlike GPR, the relation between the assumptions and the regression technique is weaker, as in LR the assumptions motivate the definition of a predictive posterior without implying it directly, and the only assumption with probabilistic content is the one about the data.

The two assumptions are the following:
\begin{itemize}[font=\itshape, leftmargin=1.5cm, align=right, labelwidth=2.4cm]
    \item[(LR-model)] the forward model $y$ is globally and component-wise Lipschitz continuous, i.e. there exist constants $L^{(1)},\dots, L^{(\text{dim}\mc Y)}$ in $\R_+^0$ such that 
    \[
        |y^{(j)}(p) - y^{(j)}(p')| \leq L^{(j)} \|p - p'\|_\Theta \ \text{ for every } \ p, p' \in \Theta
    \]
    holds for every $j = 1, \dots, \text{dim}\mc Y$;
    \item[(LR-data)] the observations are contaminated by independent and centered uniform noise, i.e. 
    \[ 
        y_i = y(p_i) + \nu_{D,i} \ \text{ for every } \ (p_i,\tau_i,y_i) \ \text{ in } \ D
    \] 
    holds, where $\nu_{D,i} \sim \mc U(I_{\tau_i})$ for some convex $I_{\tau_i} \subset \mc Y$, with $\bb E[\nu_{D,i}] = 0$ and $\nu_{D,i}$ is independent from $\nu_{D,k}$ for every $k \neq i$.
\end{itemize}  
The first assumption requires fixing a basis for $\mc Y$ and a norm on $\Theta$; despite this, the assumption is weaker than \textit{(GPR-model)}, as it does not require any level of smoothness for $y$ but Lipschitz continuity. \newline
Issues may arise due to the global nature of the Lipschitz constants $L^{(j)}$, as a model exhibiting different Lipschitz constants in different regions of the parameter space might require a great number of training points to be well-represented by LR. 
Moreover when the Lipschitz constants are unknown, their estimation is sensitive to noise.

The second assumption does not allow for correlated noise terms $\nu_{D,i}$, $\nu_{D,k}$ as in \textit{(GPR-data)}, but in practice GPR usually also assumes their independence, as in Equation~\ref{eq:GPR-noise-corr}. 
In the context of this work, \textit{(LR-data)} entails that the discretization error is uniformly distributed in a region $I_\tau$, i.e. 
\[
    H(u_{\tau}(\cdot; p)) - H(u(\cdot; p)) \sim \mc U(I_\tau),
\] 
with numerical evaluations of $y$ falling in a bounded convex region of $\mc Y$ centered around $y(p)$. \newline
Given the structure of FE discretization errors and the nature of their estimators described in Section~\ref{sec:AdaFE}, this assumption is more realistic than the one of GPR, and it is the main motivation for considering LR in this work. \medbreak

\pv{it may be not extremely clear why the below object is named a cone, and the whole proposition with proof may seem not super intuitive. if that's the case, I can add a figure depicting a 1d case which would make it much clearer.}
For any $p,p'$ in $\Theta$, $v\in \mc Y$, $\underline{L} = (L^{(1)},\dots L^{(\text{dim}\mc Y)} ) \in \R^{\text{dim} \mc Y}$ let us write 
\[
        Cone_{p,v,\underline L}(p') = \bigotimes_{j=1}^{\text{dim} \mc Y} \left[ v^{(j)} - L^{(j)} \|p-p'\|_\Theta, v^{(j)} + L^{(j)} \|p-p'\|_\Theta \right]
\]
for the projection in $p'$ of the cone in $\Omega \times \mc Y$ of vertex $(p,v)$ and directrix of slope $\underline L$.
The following proposition allows us to define a predictive interval of possible values for the target function $y$ in a point $p$.

\begin{prp}\label{prp:lips}
    Let $v \in \mc Y$, if there exists a component-wise Lipschitz function $f: \Theta \rightarrow \mc Y$ with global Lipschitz constants $\underline L = (L^{(1)},\dots L^{(\text{dim}\mc Y)} )$ such that $f(p_i) \in y_i + I_{\tau_i}$ for every $i = 1, \dots, n$ and $f(p) = v$, then it holds that
        \begin{equation} \label{eq:lips-cond1}
            Cone_{p,v,\underline L}(p_i) \cap  \big ( y_i + I_{\tau_i} \big ) \neq \emptyset \ \text{ for all } \ i = 1, \dots, n .         
        \end{equation}
    The converse holds if for each $i = 1, \dots, n$ there exist 
    \begin{equation} \label{eq:lips-cond1.2}
        \tilde y_i \in Cone_{p,v,\underline L}(p_i) \cap  \big ( y_i + I_{\tau_i} \big )
    \end{equation}
    such that
    \begin{equation} \label{eq:lips-cond2}
        \tilde y_i \in  Cone_{p_k,\tilde y_k,\underline L}(p_i) \ \text{ for all } \ k = 1, \dots, n.
    \end{equation}
\end{prp}
\begin{proof}
    Let $v\in \mc Y$ and let $f$ be as in the statement.
    For any $i\in \{1, \dots, n\}$ and $j \in \{1, \dots, \text{dim}\mc Y\}$, by Lipschitz continuity it holds 
    \[
    | v^{(j)} - f(p_i)^{(j)} | = | f(p)^{(j)} - f(p_i)^{(j)} | \leq L^{(j)} \|p-p_i\|_\Theta, 
    \] 
    which implies \[
        f(p_i)^{(j)} \in \left[ v^{(j)} - L^{(j)} \|p-p_i\|_\Theta, v^{(j)} + L^{(j)} \|p-p_i\|_\Theta \right].
    \]
    By the arbitrariness of $j$, $f(p_i) \in Cone_{p,v,\underline L}(p_i) $ and as by hypothesis $f(p_i) \in y_i + I_{\tau_i}$, we have \[
    Cone_{p,v,\underline L}(p_i) \cap  \big ( y_i + I_{\tau_i} \big ) \neq \emptyset.
    \] 
    The arbitrariness of $i$ implies the statement. \newline
    For the converse direction, let $\tilde y_1, \dots, \tilde y_n$ be as in the second part of the statement.
    We write $\tilde y_{n+1} = v$ and $p_{n+1} = p$ and then define
    \begin{equation}\label{eq:lips-interp}
        \begin{gathered}
            f:\Theta \rightarrow \mc Y \\
            p' \mapsto \frac{1}{2} \left[\min_{k=1,\dots,n+1} \left\{ \tilde y_k^{(j)} + L^{(j)} \| p' - p_k \|_\Theta \right\}+ \max_{k=1,\dots,n+1} \left\{ \tilde y_k^{(j)} - L^{(j)} \| p' - p_k \|_\Theta \right\} \right]_{j=1}^{\text{dim} \mc Y}.
        \end{gathered}
    \end{equation}
    Such $f$ satisfies the Lipschitz condition with global Lipschitz constants $\underline L$ as it scales at most linearly with the distance $\| \cdot \|_\Theta$; moreover, $f(p_i) = \tilde y_i$ for every $i = 1, \dots, n+1$.
    In fact, let $i \in \{ 1, \dots, n+1\}$: if $i<n+1$, by Condition~\eqref{eq:lips-cond1.2} we can expand Condition~\eqref{eq:lips-cond2} to $k=n+1$ obtaining
    \begin{equation}\label{eq:cone-cond}
        \tilde y_i \in Cone_{p_k, \tilde y_k,\underline L}(p_i) \ \text{ for all } k = 1, \dots, n+1;
    \end{equation}
    if $i= n+1$, we observe that or arbitrary $\hat p,p' \in \Theta$, $\hat y,y'\in \Theta$ \[
        y' \in Cone_{\hat p, \hat y, \underline L}(p') \ \text{ if and only if } \ \hat y \in Cone_{p', y', \underline L}(\hat p),
    \]
    so by Condition~\eqref{eq:lips-cond1.2} Expression~\eqref{eq:cone-cond} holds for every $i=1, \dots, n+1$. \newline
    We now fix an arbitrary $j \in \{1, \dots, \text{dim} \mc Y\}$, and write~\eqref{eq:cone-cond} for component $j$ by using the definition of the cone, obtaining
    \[
        \tilde y_i^{(j)} \in \left[ \tilde y_k^{(j)} - L^{(j)} \|p_i - p_k \|_\Theta, \tilde y_k^{(j)} + L^{(j)} \|p_i - p_k \|_\Theta \right] \ \text{ for all } k = 1, \dots, n+1,
    \] 
    which implies 
    \[
        \min_{k=1,\dots,n+1} \left\{ \tilde y_k^{(j)} + L^{(j)} \|p_i - p_k \|_\Theta \right\} =  \max_{k=1,\dots,n+1} \left\{ \tilde y_k^{(j)} - L^{(j)} \|p_i - p_k \|_\Theta \right\} = \tilde y_i^{(j)}.
    \]
    By the arbitrariness of $j$, it holds that $f(p_i) = \tilde y_i$ for every $i = 1, \dots, n+1$. \newline
    By this, we proved that there exists a Lipschitz function $f$ with global Lipschitz constants $\underline L$ such that $f(p_i) = \tilde y_i \in y_i + I_\tau$ for every $i = 1, \dots, n$ and $f(p) = v$.
\end{proof}
\begin{rmk}
    The function $f$ defined by Equation~\eqref{eq:lips-interp} is the Lipschitz interpolation of the points $\{(p_i, \tilde y_i)\}_{i=1}^n$ with global Lipschitz constants $\underline L$. 
    Lipschitz interpolation coincides with the noise-free case of LR, $\tau_i = 0$ for every $i = 1, \dots, n$ and $ I_0 = \emptyset$, and the expression of $f$ can be retrieved as a particular case of the mean prediction of the LR predictive interval, which will be defined in Definition~\ref{dfn:LR} and given with an explicit formula in Proposition~\ref{prp:LR-PI}.
\end{rmk}

The above proposition motivates the following definition.
For fixed training data $D =\{(p_i, \tau_i, y_i)\}_{i=1}^n $, noise intervals $I_{\tau_i} \subseteq \mc Y$ and Lipschitz constants $\underline L$, we write $\mc P I_p$ for the subset of $\mc Y$ containing the values $v$ which can be assumed at $p$ by some component-wise Lipschitz function with Lipschitz constants $\underline L$ and assuming value in $y_i + I_{\tau_i}$ at $p_i$, for every $i=1, \dots, n$.  \newline
By the above proposition, it holds
\begin{equation}\label{eq:LR-PI-general}
    \mc PI_p = \left \{ v \in \mc Y \mid Cone_{p,v,\underline L}(p_i) \cap  \big ( y_i + I_{\tau_i} \big ) \neq \emptyset \ \forall i = 1, \dots, n \right \},
\end{equation}
and we call $\mc P I_p$ the predictive interval in $p$.
To simplify the notation, the dipendence from the training data $D$, the noise model and the constants $\underline L$ is omitted.
\begin{rmk}
    In Proposition~\ref{prp:lips}, Condition~\eqref{eq:lips-cond2} only serves to ensure that $\mc PI_{p'}$ is non-empty for every other $p'$ in $\Theta$, i.e. that the training data is compatible with the Lipschitz assumption and an underlying globally Lipschitz function generating the data can exist. \newline
    The interesting condition for a point $p$ is Condition~\eqref{eq:lips-cond1}, which we use for the definition of the predictive interval $\mc PI_p$ and guarantees that $\mc PI_p$ includes all and only the feasible values in $p$ of Lipschitz functions compatible with the data.
\end{rmk}
\begin{rmk}
    $\mc PI_p$ inherits regularity properties from the error intervals $I_\tau$: if $I_\tau$ are convex sets, then $\mc PI_p$ is a convex set as well, and if $I_\tau$ are multi-dimensional intervals, then $\mc PI_p$ is a multi-dimensional interval as well, see Proposition~\ref{prp:LR-PI}. 
    This motivates the name predictive interval. 
\end{rmk}

The Lipschitz Regression predictive distribution is then defined by taking a uniform distribution over the predictive interval $\mc P I$.
\begin{dfn} [Lipschitz Regression] \label{dfn:LR}
    The Lipschitz regression predictive distribution given the data $D=\{ (p_i, \tau_i, y_i) \}_{i=1}^n$ and the Lipschitz constants $\underline{L} = (L^{(1)},\dots L^{(\text{dim}\mc Y)})$ is defined by
    \begin{gather*}
        y_{D, \text{LR}} : \Theta \rightarrow \mc P( \mc Y) \\
        p \mapsto \mc U( \mc PI_p ),
    \end{gather*}
    with the predictive interval $\mc PI_p$ is given by Equation~\eqref{eq:LR-PI-general}
\end{dfn}

For noise terms $\nu_{D,i}$ with independent components, i.e. $I_{\tau_i}$ being an interval, the following proposition provides a simpler expression for $\mc PI_p$, which is the one usually given for the LR prediction.
\begin{prp}\label{prp:LR-PI}
    Let $I_{\tau_i} = \otimes_{j=1}^{\text{dim}\mc Y} [ -\tau_i^{(j)}, \tau_i^{(j)} ]$ for every $i = 1, \dots, n$.
    Then the LR predictive interval $\mc PI_p$ is an interval given by
    \begin{equation}\label{eq:LR-PI}
        \mc PI_p = \bigotimes_{j=1}^{\text{dim}\mc Y} \left[ LB^{(j)}(p), UB^{(j)}(p)\right]
    \end{equation}
    with lower and upper bounds given for each component $j$ by
    \begin{equation} \label{eq:LR-bounds}
    \begin{gathered}
        LB^{(j)}(p) = \max_{i=1,\dots,n} y_i^{(j)} - \tau_i^{(j)} - L^{(j)} \| p - p_i \|_\Theta, \\
        UB^{(j)}(p) = \min_{i=1,\dots,n} y_i^{(j)} + \tau_i^{(j)} + L^{(j)} \| p - p_i \|_\Theta.
    \end{gathered}
    \end{equation}
\end{prp}
\begin{proof}
    By the definition of $\mc PI_p$ given by Equation~\eqref{eq:LR-PI-general}, we have that $v\in \mc PI_p$ if and only if for every $i = 1, \dots, n$ it holds that
    \[
    Cone_{p,v,\underline L}(p_i) \cap  \big ( y_i + I_{\tau_i} \big ) \neq \emptyset;
    \]
    by using the definition of the cone and the assumption on $I_{\tau_i}$, the above expression is equivalent to 
    \[
        \left[ v^{(j)} - L^{(j)} \|p-p_i\|_\Theta, v^{(j)} + L^{(j)} \|p-p_i\|_\Theta \right] \cap  \left[ y_i^{(j)} - \tau_i^{(j)}, y_i^{(j)} + \tau_i^{(j)} \right] \neq \emptyset
    \]
    holding for every $j = 1, \dots, \text{dim} \mc Y$. \newline
    Two intervals $[a,b]$ and $[c,d]$ have empty intersection if and only if $b < c$ or $d < a$; thus, by negation the above holds if and only if
    \[
        v^{(j)} - L^{(j)} \|p-p_i\|_\Theta \leq y_i^{(j)} + \tau_i^{(j)} \ \text{ and } \ v^{(j)} + L^{(j)} \|p-p_i\|_\Theta \geq y_i^{(j)} - \tau_i^{(j)},
    \] which is equivalent to 
    \[
        y_i^{(j)} - \tau_i^{(j)} - L^{(j)} \|p-p_i\|_\Theta \leq v^{(j)} \leq y_i^{(j)} + \tau_i^{(j)} + L^{(j)} \|p-p_i\|_\Theta.
    \]
    Consequently, we proved that $v \in \mc PI_p$ if and only if
    \[
        v^{(j)} \in \left[ y_i^{(j)} - \tau_i^{(j)} - L^{(j)} \|p-p_i\|_\Theta, y_i^{(j)} + \tau_i^{(j)} + L^{(j)} \|p-p_i\|_\Theta \right]
    \]
    for every $i = 1, \dots, n$ and $j = 1, \dots, \text{dim} \mc Y$, which implies the statement
\end{proof}

In the numerical experiments, we will adopt the LR predictive interval given by Proposition~\ref{prp:LR-PI}, as it is more computationally efficient and easier to interpret than the general one given by Equation~\eqref{eq:LR-PI-general}; nonetheless, the general form will be kept for the theoretical considerations. \medbreak

The Lipschitz constants $\underline L$ play a role similar to the one of kernel hyperparameters in GPR, as the quality of the surrogate prediction is heavily affected by their estimate.
A number of techniques have been proposed to estimate the Lipschitz constants, see~\cite{Calliess2017} for an example, but being from a Machine Learning setting they rely on an high number of training points, which is not realistic in a high-cost forward model setting.

For the independent noise components setting, the minimal value of the Lipschitz constants $L^{(j)}$ compatible with training data $D=\{ (p_i, \tau_i, y_i) \}_{i=1}^n$ is given by the following proposition.

\begin{prp}
    Let $D=\{ (p_i, \tau_i, y_i) \}_{i=1}^n$ and $y$ be a component-wise Lipschitz function with global Lipschitz constants $L^{(1)},\dots, L^{(\text{dim}\mc Y)}$ such that 
    \[
        y^{(j)}(p_i) -y_i^{(j)} \in  [-\tau_i^{(j)},\tau_i^{(j)}] \ \text{ for every } \ i = 1, \dots, n.
    \]
    Then for every component $j$ it holds that
    \begin{equation}\label{eq:LR-const}
        L^{(j)} \geq L^{(j)}_{\min}\coloneq \max_{ \stackrel{i, k \in \{1, \dots, n\}}{i\neq k} } \frac{| y_i^{(j)} - y_k^{(j)} | - \tau_i^{(j)} - \tau_k^{(j)}}{\| p_i - p_k \|_\Theta},
    \end{equation}
    and if equality holds for component $j$ then for every $t \in [0,1]$ \[
     y^{(j)}\big (t \cdot p_i + (1 - t) \cdot p_k \big )  = t \cdot \left(y_i^{(j)} - \tau_i^{(j)} \right) + (1 - t) \cdot \left(y_k^{(j)} + \tau_k^{(j)} \right)
    \]holds for the training points $i,k$, with $y^{(j)}_i > y^{(j)}_k$, realizing the maximum. 
\end{prp}
\begin{proof}
    By the Lipschitz condition, for every $p, p'$ in $\Theta$ and every $j = 1, \dots, \text{dim} \mc Y$ 
    \[
        | y^{(j)}(p) - y^{(j)}(p') | \leq L^{(j)} \| p - p' \|_\Theta
    \] holds, so by taking $p = p_i$ and $p' = p_k$ with $i \neq k$ and dividing by $\| p_i - p_k \|_\Theta$ we obtain
    \[
        L^{(j)} \geq \frac{| y^{(j)}(p_i) - y^{(j)}(p_k) |}{\| p_i - p_k \|_\Theta}.
    \]
    As $| y^{(j)}(p_h) - y_h^{(j)} | \leq \tau_h^{(j)}$ for every $h = 1, \dots, n$, we can write
    \[
        | y^{(j)}(p_i) - y^{(j)}(p_k) | = | y_i^{(j)} - y_k^{(j)} +  y^{(j)}(p_i) - y_i^{(j)} - y^{(j)}(p_k) + y_k^{(j)} |  \geq | y_i^{(j)} - y_k^{(j)} | - \tau_i^{(j)} - \tau_k^{(j)},
    \]
    which by the arbitrariness of $i$ and $k$ implies Equation~\eqref{eq:LR-const}. \newline
    For the second part, let $j$ be such that $L^{(j)} = L^{(j)}_{\min}$ and let $i,k$ be the training points realizing the maximum in Equation~\eqref{eq:LR-const}, with $y^{(j)}_i > y^{(j)}_k$. \newline
    We prove second part of the statement by using Proposition~\ref{prp:lips} and Proposition~\ref{prp:LR-PI} : let $t \in [0,1]$ and $p = t \cdot p_i + (1 - t) \cdot p_k$, then by the first cited result we have that $y(p) \in \mc PI_p$. 
    By the second cited result, taking the $j$-th component of the predictive interval deals
    \begin{equation}\label{eq:proof-LR-elem0}
        y^{(j)}_h - \tau_h^{(j)} - L^{(j)} \| p - p_h \|_\Theta \leq y^{(j)}(p) \leq y^{(j)}_l + \tau_l^{(j)} + L^{(j)} \| p - p_l \|_\Theta
    \end{equation}
    for every $h,l \in \{1, \dots, n\}$. 
    We note that 
    \begin{equation}\label{eq:proof-LR-elem1}
        L^{(j)}\| p_i - p_k \|_\Theta =  y_i^{(j)} - y_k^{(j)} - \tau_i^{(j)} - \tau_k^{(j)}
    \end{equation}
    holds and by $p = t \cdot p_i + (1 - t) \cdot p_k$ we have that 
    \begin{gather}
        \| p - p_i \|_\Theta = (1-t) \cdot \| p_i - p_k \|_\Theta \ \text{ and} \label{eq:proof-LR-elem2} \\
        \| p - p_k \|_\Theta = t \cdot \| p_i - p_k \|_\Theta \label{eq:proof-LR-elem3}
    \end{gather}
     hold.\newline
    These observations deal 
    \begin{flalign*}
        y_i - & \tau_i - L^{(j)} \| p - p_i \|_\Theta \stackrel{\ref{eq:proof-LR-elem2}}{=} y_i^{(j)} - \tau_i^{(j)} - L^{(j)} (1-t) \cdot \| p_i - p_k \|_\Theta \stackrel{\ref{eq:proof-LR-elem1}}{=}  && \\
        &\stackrel{\ref{eq:proof-LR-elem1}}{=} y_i^{(j)} - \tau_i^{(j)} - (1-t) \cdot \left(y_i^{(j)} - y_k^{(j)} - \tau_i^{(j)} - \tau_k^{(j)}\right)=  t \cdot \left(y_i^{(j)} - \tau_i^{(j)} \right) + (1 - t) \cdot \left(y_k^{(j)} + \tau_k^{(j)} \right) =   &&\\ 
        &= y_k^{(j)} + \tau_k^{(j)} + t \cdot \left(y_i^{(j)} - y_k^{(j)} - \tau_i^{(j)} - \tau_k^{(j)}\right) \stackrel{\ref{eq:proof-LR-elem1}}{=} y_k^{(j)} + \tau_k^{(j)} + t \cdot L^{(j)} \| p_i - p_k \|_\Theta \stackrel{\ref{eq:proof-LR-elem3}}{=} && \\ 
        &\stackrel{\ref{eq:proof-LR-elem3}}{=} y_k^{(j)} + \tau_k^{(j)} + L^{(j)} \| p - p_k \|_\Theta .
    \end{flalign*}
    By this chain of equalities, we have that for $h=i$ and $l=k$ the two sides of the inequalities in Equation~\eqref{eq:proof-LR-elem0} are equal to the right-hand-side of the desired equality and so does $y^{(j)}(p)$, concluding the proof.
\end{proof}
\begin{rmk}
    Unless $L^{(j)}_{\min}< 0$ for some $j$, the lower bound $L^{(j)}=L^{(j)}_{\min}$ for every $j$ can be realized, for instance by a Lipschitz interpolation function, so the bound provided by Equation~\eqref{eq:LR-const} is optimal. 
\end{rmk}
\begin{cor} \label{cor:LR-const}
    Let $D=\{ (p_i, \tau_i, y_i) \}_{i=1}^n$ be some training data, and $L^{(1)},\dots, L^{(\text{dim}\mc Y)}$ be Lipschitz constants. \newline
    In order for the training data to be feasible, i.e. it can be generated by inexact evaluations of a Lipschitz function with global Lipschitz constants $L^{(1)},\dots, L^{(\text{dim}\mc Y)}$ and the predictive interval $\mc IP_p$ is non-empty for every $p\in \Theta$, it is necessary that Equation~\eqref{eq:LR-const} holds for all $j = 1, \dots, \text{dim} \mc Y$.\\
    If $L^{(j)} = L^{(j)}_{\min}$ holds for some $j$, then
    \begin{flalign}\label{eq:LR-const-minimality}
        LB^{(j)}\big (t \cdot p_i + (1 - t) \cdot p_k \big ) &=UB^{(j)}\big (t \cdot p_i + (1 - t) \cdot p_k \big ) = && \notag \\
        & = t \cdot \left(y_i^{(j)} - \tau_i^{(j)} \right) + (1 - t) \cdot \left(y_k^{(j)} + \tau_k^{(j)} \right) \ \text{ for every } \ t \in [0,1] &&
    \end{flalign}
    holds for the training points $i,k$, with $y^{(j)}_i > y^{(j)}_k$, realizing the maximum.
\end{cor}
\begin{rmk}
By the minimality condition~\eqref{eq:LR-const-minimality}, if at component $j$ the lower bound is obtained with training points $p_i, \ p_k$, then the $j$-th components of the predictive interval $\mc PI_{p}$ degenerates into a single point in all the points $p = t \cdot p_i + (1-t) \cdot p_k$ for $t \in [0,1]$. 
\end{rmk}

Corollary~\ref{cor:LR-const} suggests an approach to estimate the Lipschitz constants: for training data $D=\{ (p_i, \tau_i, y_i) \}_{i=1}^n$ the Lipschitz constants estimate is given by 
\begin{equation}\label{eq:lips-const}
    \hat{\underline{L}}_D = \lambda ( L^{(1)}_{\min}, \dots, L^{(\text{dim}\mc Y)}_{\min})
\end{equation} for some fixed $\lambda > 1$. 
The multiplicative constant $\lambda$ is considered to avoid the degenerate case given by Equation~\eqref{eq:LR-const-minimality}, and we will set its value in the experimental section.