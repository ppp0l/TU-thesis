\section{Stochastic surrogate models}\label{sec:surrogates}
To reduce the computational costs related to the evaluation of the forward model, a surrogate model is an attractive option.
The scope of the surrogate model is to approximate the forward model $y : \Theta \rightarrow \mc Y $ or the problem's likelihood $L: \Theta \rightarrow \R $ directly with a simpler and faster model.\medbreak

Whether the surrogate aims at representing the forward model or the likelihood directly is a matter of convenience.
Producing a surrogate representation of the forward model guarantees a cheaper representation of the likelihood, but not vice versa. Moreover, the values the likelihood assumes are constrained to be positive, which can be non-trivial to impose for certain classes of surrogates; however, an argument in favor of adopting a surrogate for the likelihood is that, if the forward model has a great number of output components, the computational costs of a likelihood surrogate can be significantly lower than the ones of a forward model surrogate.

Ultimately, the choice of the target of the surrogate model depends on the adopted surrogating technique, the usage of the surrogate and the objective being pursued.
When a point estimate of the parameter is a sufficient solution for the IP, adopting a Bayesian Optimization framework and surrogating the likelihood directly is the most natural choice.
Instead, when aiming for a posterior representation through sampling, approaches such as Delayed Acceptance MCMC \cite{ChristenFox2005} are indifferent to the choice.
For the approach presented in this work, a forward model representation will be obtained, as we will exploit the error estimators for FE models in the surrogate's training. \medbreak

We will consider training data $ D =\{ (p_i, \tau_i, y_i) \}_{i=1}^n $, where $ p_i \in \Theta $ are the input parameters, $ \tau_i \in \R_+ $ are the tolerances of the model's evaluation, and $ y_i \in \mc Y $ are the output values of the numerical forward model,\[ y_i = H(f_{\tau_i}(\cdot; p_i)). \]

There exist different classes of surrogate models: an useful distinction is the one between deterministic and stochastic surrogate models.
Deterministic surrogate models are based on deterministic functions, such as polynomial regression, neural networks, or support vector machines.
They provide an approximation of the forward model or the likelihood, but they do not provide by default an estimate of their own uncertainty.
On the other hand, stochastic surrogate models are based on probabilistic assumptions and naturally provide an estimate of the uncertainty associated with their prediction.
This is particularly useful in the context of BIPs, as the uncertainty of the surrogate model can be incorporated in to the problem's likelihood, as illustrated in Sections~\ref{sec:GPlike} and~\ref{sec:LRlike}. \medbreak

We will adopt stochastic surrogate models, which for any $p \in \Theta$ provide a random prediction with a certain predictive distribution. 
The surrogate model could thus be viewed as a stochastic process, but as this is not necessary for the scopes of this work, we see it as a function that to each parameter $p$ associates a probability distribution $ P( y(p) \mid D)$ in the set $\mc P (\mc Y)$ of probability distributions over $\mc Y$.
For training data $D$, we write \begin{align*}
    y_D : \Theta \rightarrow \mc P( \mc Y) \\
    p \mapsto \bb P( y(p) \mid D).
\end{align*}

In the next two section, we introduce two techinques which provide a stochastic surrogate model: Gaussian process regression (GPR) and Lipschitz regression (LR).

\subsection{Gaussian process regression}\label{sec:GPR}
Gaussian process regression (GPR) is a non-parametric regression technique which relies on probabilistic assumptions and is closely related to support vector machines and kernel methods.

The core idea is that the underlying function $y$ is a realization of a Gaussian process and, assuming the observation are contaminated by Gaussian noise, GPR obtains a closed-form prediction through Gaussian conditioning.\medbreak

\begin{dfn} [Gaussian Process]
    A Gaussian process (GP) over $\mc Y$ is a collection $Y = \{Y_p\}_{p\in\Theta}$ of $\mc Y$-valued random variables, such that for any finite subset $p_1, \ldots, p_n \in \Theta$, the random variables $Y_{p_1}, \ldots, Y_{p_n}$ have a joint Gaussian distribution.

    A Gaussian process is completely defined by its mean function $m: \Theta \rightarrow \mc Y$ and its covariance function $k: \Theta \times \Theta \rightarrow SPSD_{\text{dim}\mc Y}(\R)$, also known as kernel. 
    We denote $Y \sim GP(m, k)$ for a Gaussian process of mean $m$ and kernel $k$.
\end{dfn}
Note that the covariance operator $k(p, p')$ can be identified with a positive semidefinite matrix as $\mc Y$ is of finite dimension. \medbreak

Gaussian process regression relies on two assumptions:
\begin{itemize}[font=\itshape, leftmargin=1.5cm, align=right, labelwidth=2.4cm]
    \item[(GPR-model)] the forward model $y$ is a realization of a GP $Y \sim GP(m, k)$;
    \item[(GPR-data)] the observations are contaminated by Gaussian noise indipendent from $Y$, i.e. 
    \[ 
        y_i = y(p_i) + e_{D,i} \ \text{ for every } \ (p_i,\tau_i,y_i) \ \text{ in } \ D 
    \] 
    holds, where $e_{D,i} \sim \mc N(0, \Sigma_D(\tau_i)^2)$ for some $\Sigma_D(\tau)$, and $e_{D,i}$ is indipendent from $Y$.
\end{itemize}  
The first assumption implicitly prescribes a certain regularity of the forward model depending on the choice of the kernel $k$, but does not impose any parametric form on the model itself. 
The choice of a kernel is crucial for the effectiveness of GPR: in order to allow for flexibility, in practice a parametric family of kernels $\{ k_\lambda\}$ is chosen to model the covariance, with $\lambda$ known as the kernel's hyperparameters. More remarks about kernel structure and hyperparameters will be made at the end of this section.

The second assumption allows to derive a closed-form expression for the predictive distribution of the GP, and in our context entails that the discretization error on numerical evaluations of the forward model is Gaussian, i.e. 
\[
    H(f_{\tau}(\cdot; p)) - H(f(\cdot; p)) \sim \mc N(0, \Sigma_D(\tau)^2);
\] for a general GPR task, it is common to assume 
\begin{equation}\label{eq:iid-GPR-noise}
    \Sigma_D(\tau) = \tau I_{\text{dim} \mc Y}
\end{equation} 
and often $\tau$ is unknown and fitted together with the kernel's hyperparameters. \newline
In the context we are working in, Equation~\eqref{eq:iid-GPR-noise} implies i.i.d. components of the FE discretization error, which is not realistic: Section~\ref{sec:GPAL} presents an approach to model the discretization error covariance by exploiting an adaptive FE scheme. \medbreak

Given a prior GP $Y \sim GP(m, k)$ and the training data $D$, the posterior GP $Y \mid D$ is obtained by conditioning the prior on the observations. 
The analytical expression for the posterior GP is given by the following theorem.
Although this result is foundational for GPR, its complete proof is rarely explicitly stated.~\cite[Appendix A]{RasmussenWilliams2006} provides the necessary results for the full proof, which we will present explicitly in Appendix~\ref{app:GPR}.

\begin{restatable}[GP posterior]{theorem}{GPpost} \label{thm:GP-posterior}
    Under assumptions \textit{(GPR-model)} and \textit{(GPR-data)}, assume that the covariance between the noise terms $e_{D,1},\dots,e_{D,n}$ is given by some positive definite $\Sigma_D(\mc T)^2$ for $\mc T = (\tau_1, \ldots, \tau_n)$. \newline
    Then the posterior GP $Y \mid D$ is a GP over $\mc Y$ with mean function $m_D$ and kernel $k_D$, where
    \begin{gather}
        m_D(p) = m(p) + k(p, P) (k(P,P) + \Sigma_D(\mc T)^2 )^{-1} (y - m(P)) \label{eq:predictive-mean} \\
        k_D(p, p') = k(p, p') - k(p, P) (k(P,P) + \Sigma_D(\mc T)^2 )^{-1} k(P, p')^T \label{eq:predictive-var}
    \end{gather}
    for $P = (p_1, \ldots, p_n)$ and block matrices $K(P,P)_{i,j} = k(p_i, p_j)$, and $k(p, P) = (k(p, p_1), \ldots, k(p, p_n))$.
\end{restatable}

As we are not interested in the full GP posterior but only in the point-wise predictive distribution, the following corollary allows us to define the GPR surrogate we will employ.
\begin{cor}
    Under the assumptions of Theorem~\ref{thm:GP-posterior}, the GPR predictive distribution given the prior $GP(m,k)$ and the data $D$, is given by 
    \begin{gather*}
        y_{D, \text{GP}} : \Theta \rightarrow \mc P( \mc Y) \\
        p \mapsto \mc N( m_D(p), k_D(p,p) ),
    \end{gather*}
    with $m_D$ and $k_D$ are given by Equations~\eqref{eq:predictive-mean} and~\eqref{eq:predictive-var} respectively.
\end{cor}

The core of GPR is now outlined, but for practical use a few more remarks are necessary. \newline
First, note that the covariance between the noise terms $e_{D,i}, \ e_{D,k}$ is assumed to be known in Theorem~\ref{thm:GP-posterior}, but in practice information about correlations is scarce or absent. Thus, usually a block diagonal structure is assumed 
\begin{equation} \label{eq:GPR-noise-corr}
    \Sigma_D(\mc T)^2 = \text{diag}(\Sigma_D(\tau_1)^2, \ldots, \Sigma_D(\tau_n)^2),
\end{equation}
entailing uncorrelated noise terms.

Moreover as mentioned above, the kernel structure and the treatment of hyperparameters play an essential role in GPR.

The choice of what parametric family of kernels $\{ k_\lambda\}$ to adopt determines the regularity of the GP's mean and realizations, see ~\cite{Kanagawa2018} and~\cite[chap. 4]{ChristmannSteinwart2008}; moreover, the parametrization $\lambda \mapsto k_\lambda$ needs to allow for efficient optimization of the hyperparameters $\lambda$. \newline
To ease the hyperparameters optimization problem and improve computational efficiency, the kernel $k_\lambda$ is often chosen to be stationary, i.e. 
\[
k_\lambda(p, p') = g_\lambda(p - p') \ \text{ for every } \ p, p' \in \Theta
\]
for some $g_\lambda:\Theta \rightarrow \R$.
For dim$\mc Y > 1$, it is common to model the correlation between inputs and outputs separately~\cite{AlvarezRosascoLawrence2012}; this results in what is known as a separable kernel \[
    k_\lambda(p, p') = \kappa_{\lambda_i}(p, p') K_{\lambda_o}
\] for $\lambda = (\lambda_i, \lambda_o)$, with $\kappa_{\lambda_i}: \Omega \times \Omega \rightarrow \R$ and $K_{\lambda_o} \in R^{m\times m}$ being a scalar kernel and a symmetric positive definite matrix.
$\kappa_{\lambda_i}$ models the correlation between different inputs and $K_{\lambda_o}$ models the correlation between output components.

The possible choices for families of kernels are vast and it is important to note that sums and products of kernels are again kernels, allowing for the construction of complex kernel structures by the combination of elementary kernels~\cite{Duvenaud}.
Among such elementary kernels, particularly relevant are the Radial Basis Function (RBF) kernel and the Matérn kernel. \newline
The expression of the RBF kernel, also known as squared exponential kernel, is given by
\[
k_{\lambda}(p, p') = \exp\left(-\frac{\|p - p'\|_\Theta^2}{2\lambda^2}\right),
\]
with $\lambda$ being a length-scale parameter. 
Note that more complex RBF kernels can be obtained by altering the norm $\| \cdot \|_\Theta$, and for a parametrized norm this leads to what is known as Automatic Relevance Determination (ARD).
The RBF kernel leads to a $\mc C^\infty$ mean prediction and is usually adopted as a default choice whenever the underlying function can be assumed to be rather smooth. \newline
The Matérn kernel allows for less-regular predictions. Its general expression is given by
\[
k_{(\nu, \lambda)}(p, p') = \frac{2^{1-\nu}}{\Gamma(\nu)} \left( \frac{\sqrt{2\nu} \|p - p'\|}{\lambda} \right)^\nu K_\nu \left( \frac{\sqrt{2\nu} \|p - p'\|}{\lambda} \right),
\]
where $\Gamma$ is the Gamma function $K_\nu$ is a modified Bessel function~\cite[sec. 9.6]{AbramowitzStegun1964};
for $\nu = n + \frac{1}{2}$ with $n\in \bb N$, the expression simplifies to 
\[
     k_{n+\frac{1}{2}, \lambda}(p, p')=\exp \left(-{\frac {{\sqrt {2n+1}}\|p - p'\|}{\lambda }}\right){\frac {n!}{(2n)!}}\sum _{i=0}^{n}{\frac {(n+i)!}{i!(n-i)!}}\left({\frac {2{\sqrt {2n+1}}\|p - p'\|}{\lambda }}\right)^{n-i}.
\]
The regularity of the predictive mean depends on the parameter $\nu$: the mean prediction is $\nu + \frac{1}{2}$-times weakly differentiable.
Most commonly, $\nu$ is set to $\frac{3}{2}, \frac{5}{2}$ or $\frac{7}{2}$, as $\nu = \frac{1}{2}$ leads to a non-differentiable mean prediction, and for $\nu > \frac{7}{2}$ the high regularity favors the adoption of a RBF kernel.
Note that for $\nu \rightarrow \infty$ the Matérn kernel converges to the RBF kernel. \medbreak

The treatment of hyperparameters constitutes an estimation problem.
For training data $D =\{ (p_i, \tau_i, y_i) \}_{i=1}^n$, the distribution of the data given the hyperparameters $\Lambda = \lambda$ is given by
\[
    (Y_{p_1}, \dots, Y_{p_n}) \mid \Lambda \sim \mc N \left( (m(p_1), \dots, m(p_n)), k_\Lambda(P,P) + \Sigma_D(\mc T)^2 \right)
\]
with $k(P,P)$ and $\Sigma_D(\mc T)$ as given in Theorem~\ref{thm:GP-posterior};
this results in the hyperparameters' likelihood given the data,
\[
    L(\lambda) = \pi_{(Y_{p_1}, \dots, Y_{p_n}) \mid \Lambda} \big ( (y_1,\dots,y_n) \mid \Lambda = \lambda \big ).
\]
To solve the problem, one either adopts a point estimate, given usually by ML or MAP, or marginalizes the GPR prediction over the hyperparameter posterior distribution by sampling. 
While marginalizing propagates the uncertainty in the hyperparameters, it is computationally considerably more expensive: due to this reason a point estimate is most commonly employed and one talks about hyperparameter optimization. Gradient-based methods are utilized in the problem's solution. \medbreak


\subsection{Lipschitz regression}\label{sec:LR}
Lipschitz regression (LR) is a non-parametric regression technique which provides uniform confidence intervals and hard bounds for the values of the target function. \medbreak

Similarly to GPR, LR depends on one assumption about the model and on one assumption about the data; unlike GPR, the relation between the assumptions and the regression technique is weaker, as in LR the assumptions motivate the definition of a predictive posterior without implying it directly, and the only assumption with probabilistic content is the one about the data.

The two assumptions are the following:
\begin{itemize}[font=\itshape, leftmargin=1.5cm, align=right, labelwidth=2.4cm]
    \item[(LR-model)] the forward model $y$ is globally and component-wise Lipschitz continuous, i.e. there exist constants $L^{(1)},\dots, L^{(\text{dim}\mc Y)}$ in $\R_+^0$ such that 
    \[
        |y^{(j)}(p) - y^{(j)}(p')| \leq L^{(j)} \|p - p'\|_\Theta \ \text{ for every } \ p, p' \in \Theta
    \]
    holds for every $j = 1, \dots, \text{dim}\mc Y$;
    \item[(LR-data)] the observations are contaminated by indipendent and centered uniform noise, i.e. 
    \[ 
        y_i = y(p_i) + e_{D,i} \ \text{ for every } \ (p_i,\tau_i,y_i) \ \text{ in } \ D
    \] 
    holds, where $e_{D,i} \sim \mc U(I_{\tau_i})$ for some convex $I_{\tau_i} \subset \mc Y$, with $\bb E[e_{D,i}] = 0$ and $e_{D,i}$ is indipendent from $e_{D,k}$ for every $k \neq i$.
\end{itemize}  
The first assumption requires fixing a basis for $\mc Y$ and a norm on $\Theta$; despite this, the assumption is weaker than \textit{(GPR-model)}, as it does not require any level of smoothness for $y$ but Lipschitz continuity. \newline
Issues may arise due to the global nature of the Lipschitz constants $L^{(j)}$, as a model exihibiting different Lipschitz constants in different regions of the parameter space might require a great number of training points to be well-represented by LR. 
Moreover when the Lipschitz constants are unknown, their estimation is sensitive to noise.

The second assumption does not allow for correlated noise terms $e_{D,i}$, $e_{D,k}$ as in \textit{(GPR-data)}, but in practice GPR usually also assumes their indipendence, as in Equation~\ref{eq:GPR-noise-corr}. 
In the context of this work, \textit{(LR-data)} entails that the discretization error is uniformly distributed in a region $I_\tau$, i.e. 
\[
    H(f_{\tau}(\cdot; p)) - H(f(\cdot; p)) \sim \mc U(I_\tau),
\] 
with numerical evaluations of $y$ falling in a bounded convex region of $\mc Y$ centered around $y(p)$. \newline
Given the structure of FE discretization errors and the nature of their estimators described in Section~\ref{sec:AdaFE}, this assumption is more realistic than the one of GPR, and it is the main motivation for considering LR in this work. \medbreak

The following proposition allows us to define a predictive interval of possible values for the target function $y$ in a point $p$.
\begin{prp}
    Let $\upsilon \in \mc Y$, there exists a component-wise Lipschitz function $f: \Theta \rightarrow \mc Y$ with gloobal Lipschitz constants $\underline L = (L^{(1)},\dots L^{(\text{dim}\mc Y)} )$ such that $f(p_i) \in y_i + I_{\tau_i}$ for every $i = 1, \dots, n$ and $f(p) = \upsilon$ if and only if $\upsilon \in \mc C_p$, with $\mc PI_p$ given by
    \begin{equation}\label{eq:LR-PI-general}
        \mc PI_p = \left \{ \upsilon \in \mc Y \mid Cone_{p,\upsilon,\underline L}(p_i) \cap  \big ( y_i + I_{\tau_i} \big ) \neq \emptyset \ \forall i = 1, \dots, n \right \}.
    \end{equation}
\end{prp}
\begin{proof}
    \todo[inline]{Proof}
\end{proof}
\begin{rmk}
    $\mc PI_p$ inherits regularity properties from the error intervals $I_\tau$: if $I_\tau$ are convex sets, then $\mc PI_p$ is a convex set as well, and if $I_\tau$ are multi-dimensional intervals, then $\mc PI_p$ is a multi-dimensional interval as well, see Proposition~\ref{prp:LR-PI}. 
    This motivates the name predictive interval. 
\end{rmk}

The Lipschitz Regression predictive distribution is then defined by taking a uniform distribution over the predictive interval $\mc P I$.
\begin{dfn} [Lipschitz Regression] \label{dfn:LR}
    The Lipschitz regression predictive distribution given the data $D=\{ (p_i, \tau_i, y_i) \}_{i=1}^n$ and the Lipschitz constants $\underline{L} = (L^{(1)},\dots L^{(\text{dim}\mc Y)})$ is defined by
    \begin{gather*}
        y_{D, \text{LR}} : \Theta \rightarrow \mc P( \mc Y) \\
        p \mapsto \mc U( \mc PI_p ),
    \end{gather*}
    with the predictive interval $\mc PI_p$ is given by Equation~\eqref{eq:LR-PI-general}
\end{dfn}

For noise terms $e_{D,i}$ with indipendent components, i.e. $I_{\tau_i}$ being an interval, the following proposition provides a simpler expression for $\mc PI_p$, which is the one usually given for the LR prediction.
\begin{prp}\label{prp:LR-PI}
    Let $I_{\tau_i} = \otimes_{j=1}^{\text{dim}\mc Y} [ -\tau_i^{(j)}, \tau_i^{(j)} ]$ for every $i = 1, \dots, n$.
    Then the LR predictive interval $\mc PI_p$ is an interval given by
    \begin{equation}\label{eq:LR-PI}
        \mc PI_p = \bigotimes_{j=1}^{\text{dim}\mc Y} \left[ LB^{(j)}(p), UB^{(j)}(p)\right]
    \end{equation}
    with lower and upper bounds given for each component $j$ by
    \begin{equation} \label{LR-bounds}
    \begin{gathered}
        LB^{(j)}(p) = \max_{i=1,\dots,n} y_i^{(j)} - \tau_i^{(j)} - L^{(j)} \| p - p_i \|_\Theta, \\
        UB^{(j)}(p) = \min_{i=1,\dots,n} y_i^{(j)} + \tau_i^{(j)} + L^{(j)} \| p - p_i \|_\Theta.
    \end{gathered}
    \end{equation}
\end{prp}
\begin{proof}
    \todo[inline]{Proof}
\end{proof}

In the numerical experiments, we will adopt the LR predictive interval given by Proposition~\ref{prp:LR-PI}, as it is more computationally efficient and easier to interpret than the general one given by Equation~\eqref{eq:LR-PI-general}; nonetheless, the general form will be kept for the theoretical considerations. \medbreak

The Lipschitz constants $\underline L$ play a role similar to the one of kernel hyperparameters in GPR, as the quality of the surrogate prediction is heavily affected by their estimate.
A number of techniques have been proposed to estimate the Lipschitz constants, see~\cite{Calliess2017} for an example, but being from a Machine Learning setting they rely on an high number of training points, which is not realistic in a high-cost forward model setting.

For the indipendent noise components setting, the minimal value of the Lipschitz constants $L^{(j)}$ compatible with training data $D=\{ (p_i, \tau_i, y_i) \}_{i=1}^n$ is given by the following proposition.
\begin{prp} \label{prp:LR-const}
    Let $D=\{ (p_i, \tau_i, y_i) \}_{i=1}^n$ and $y$ be a component-wise Lipschitz function with global Lipschitz constants $L^{(1)},\dots, L^{(\text{dim}\mc Y)}$ such that 
    \[
        y^{(j)}(p_i) -y_i^{(j)} \in  [-\tau_i^{(j)},\tau_i^{(j)}] \ \text{ for every } \ i = 1, \dots, n.
    \]
    Then for every component $j$ it holds that
    \begin{equation}\label{eq:LR-const}
        L^{(j)} \geq L^{(j)}_{\min}\coloneq \max_{ \stackrel{i, k \in \{1, \dots, n\}}{i\neq k} } \frac{| y_i^{(j)} - y_k^{(j)} | - \tau_i^{(j)} - \tau_k^{(j)}}{\| p_i - p_k \|_\Theta},
    \end{equation}
    and equality holds if and only if
    \begin{equation}\label{eq:LR-const-minimality}
        \begin{aligned}
            LB^{(j)}(p_i) =UB^{(j)}(p_i) = y_i^{(j)} + \tau_i^{(j)}  \ &\text{ and }   \ LB^{(j)}(p_k) =UB^{(j)}(p_k) = y_k^{(j)} - \tau_k^{(j)}, \ \text{ or } \\
            LB^{(j)}(p_i) =UB^{(j)}(p_i) = y_i^{(j)} - \tau_i^{(j)}  \ &\text{ and }   \ LB^{(j)}(p_k) =UB^{(j)}(p_k) = y_k^{(j)} + \tau_k^{(j)}
        \end{aligned}
    \end{equation}
    hold for the training points $i,k$ realizing the maximum. 
\end{prp}
\begin{proof}
    \todo[inline]{Proof}
\end{proof}
\begin{rmk}
Due to the fact the $L^{(j)}=L^{(j)}_{\min}$ can be realized, the bound provided by Equation~\eqref{eq:LR-const} is optimal. 
\end{rmk}
\begin{rmk}
By the minimality condition~\eqref{eq:LR-const-minimality}, if at component $j$ the lower bound is obtained with training points $p_i, \ p_k$, then the $j$-th components of the predictive intervals $\mc PI_{p_i},\ \mc PI_{p_k}$ degenerate into a single point; it can be proved that this happens in all the points $p = t p_i + (1-t) p_k$ for $t \in [0,1]$. 
\end{rmk}

Proposition~\ref{prp:LR-const} suggests an approach to estimate the Lipschitz constants: for training data $D=\{ (p_i, \tau_i, y_i) \}_{i=1}^n$ the Lipschitz constants estimate is given by $\hat{\underline{L}}_D = \lambda ( L^{(1)}_{\min}, \dots, L^{(\text{dim}\mc Y)}_{\min})$ for some fixed $\lambda > 1$. 
The multiplicative constant $\lambda$ is considered to avoid the degenerate case given by Equation~\eqref{eq:LR-const-minimality}, and we will set its value in the experimental section.