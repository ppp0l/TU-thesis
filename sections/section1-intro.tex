\section{Introduction} \label{sec:intro}

When dealing with real-world phenomena, adopting a mathematical representation is often necessary in order to simplify complexity, obtain insight about underlying features, and produce sufficiently accurate predictions on the behaviour of the considered portion of reality. 

In the present day, computer simulations and scientific computing have become essential parts of mathematical modeling, rendering it possible to adopt numerical techniques to solve analytically untractable problems.
Moreover, as numerous and vast families of mathematical models are available to adequately represent different classes of processes, the modeler is presented with choices about model complexity and desired characteristics, as well as with tasks such as parameter identification, model order reduction and uncertainty quantification. 
This work involves all three of these tasks in different manners and measures. 

The problem of the calibration of parameters for a computationally expensive numerical model is considered: a Bayesian perspective is adopted in formulating the inverse problem, and then the cost of model evaluation is reduced by considering a regression-based surrogate model.
The considered regression techniques are Gaussian process regression (GPR), which is commonly employed in Bayesian Inverse Problems as it provides not only a pointwise estimate but also a quantification of its confidence, and Lipschitz regression (LR), which is a less utilized technique that also provides a stochastic prediction but relies on different assumptions than GPR. 
To further enhance the saving of computational resources an adaptive training strategy for the surrogates is proposed, aiming at the selection not only of optimal positions for the training points but also of optimal evaluation tolerances.

To understand the motivation behind this work, it is necessary to provide a brief introduction to the setting in which it is situated.

\subsection{Context and motivation}\label{sec:context}

Inverse Problems (IP) involve the identification of some unidentified quantities in a model through a number of measurements.

We consider a finite dimensional parameter space $\Theta$, a finite dimensional measurement space $ \mc Y$ and the parameter-to-observation map 
\[
 y : \Theta \longrightarrow \mc Y, 
\]
which to each parameter value associates the corresponding measured quantity.

The map implicitly depends on an associated model: we assume the underlying model to be a Partial Differential Equation (PDE) model, which can be evaluated through a Finite Element (FE) discretization.
For a parametrized solution $u(\cdot;p)$ with $p\in \Theta$, some measurement operator $H$ projects the infinite dimensional solution $u$ into the finite dimensional measurement, 
\[
    y (p) = H(u(\cdot;p)) \in \mc Y.
\]
As the PDE solution $u$ is only available through an inexact numerical evaluation $u_\tau$ up to some tolerance $\tau$, the forward model can only be evaluated approximately, i.e.
\[
    y_\tau (p) = H(u_\tau(\cdot;p)) \approx y(p).
\]

FE simulations are often computationally expensive: in order to reduce computational costs, a surrogate model approximating $y$ is introduced.
For a training set $D = \{(p_i,y_i,\tau_i)\}_{i=1}^n$, with $y_i = H(u_{\tau_i}(\cdot;p_i))$, both GPR and LR can provide a mean prediction and some stochastic error bound. 

GPR assumes a Gaussian process prior $\mc{G} \mc P (\mu, k)$ for the unknown ground truth $y$, with $\mu : \mc P \rightarrow \mc Y$ being some mean function and $k: \mc P \times \mc P \rightarrow \mc Y^* $ being a covariance kernel.
Moreover, it assumes that the error on the numerical evaluations $y_\tau$ is a realization of some normally distributed noise, i.e. 
\begin{equation}\label{eq:GPR-noise}
    y_\tau(p) \ \text{ is a realization of } \ y(p) + \nu_d,  \ \text{ with } \ \nu_d \sim \mc N (0, \Sigma_d^2(\tau)),    
\end{equation}
for some covariance matrix $\Sigma_d^2(\tau)$ which depends on the tolerance $\tau$, with a greater tolerance corresponding to larger variance components.
Thanks to this assumptions, it obtains a predictive Gaussian distribution for unseen values of $y$, and computable analytical formulas for the mean and the variance are given.

LR does not need any prior distributional assumptions on $y$, but instead assumes that $y$ is a Lipschitz function.
The Lipschitz constant can be assumed or estimated from data.
Further, the numerical evaluation error is assumed to be the effect of uniformly distributed noise,  
\begin{equation}\label{eq:LR-noise}
    y_\tau(p) \ \text{ is a realization of } \ y(p) + \nu_d,  \ \text{ with } \ \nu_d \sim U(I_\tau )    
\end{equation}
where $I_\tau$ is an open, connected and centered subset of $\mc Y$ which depends on $\tau$.
With these assumptions, LR provides a predictive uniform distribution for unseen values of $y$, guaranteeing a mean prediction and hard bounds.\medskip

The motivation for considering LR lies precisely in the different assumptions on the evaluation noise, Assumptions~\ref{eq:GPR-noise} and~\ref{eq:LR-noise}, required by the two techniques.
For an underlying FE model, the evaluation error depends mostly on the discretization error which results from the choice of a FE basis.
For adaptive FE, the basis is refined until the prescribed tolerance $\tau$ is not met by an error indicator.
The error indicator often guarantees an hard bound on the discretization error: while this is compatible with Assumption~\ref{eq:LR-noise}, it is incompatible with Assumption~\ref{eq:GPR-noise}, which requires an unbounded support for the evaluation error. 

Additionally, in Assumption~\ref{eq:GPR-noise} the covariance matrix $\Sigma_d(\tau)^2$ is often assumed to be diagonal, resulting in independent noise components. 
This ignores any correlation between error components, while the discretization error has some structure due to the underlying FE system.
Hoping to improve the quality of GPR's prediction we introduce and adapt to the given context linear shrinkage estimators, aiming at recovering the underlying error structure. \medskip

For surrogates of both kinds, the IP is reformulated in order to account for the uncertainty introduced by the surrogate model.
A marginal likelihood is derived for both GPR and LR surrogates, and a surrogate-based posterior distribution is obtained.
To select training points, we consider an adaptive training strategy which aims at spending the least computational budget possible in order to obtain a surrogate model with a given accuracy under a certain metric.
We adopt a sequential Design of Experiments (DoE) approach, where the training set is iteratively updated by adding new training points $(p,\tau,y)$, with $p$ being the position of the training point, $\tau$ being the evaluation tolerance and $y$ being the corresponding numerical evaluation.
The considered strategy is goal-oriented, meaning that the training set is adapted to the IP task at hand; it is adaptive, in the sense that not only the position $p$ of the training points $(p,\tau,y)$ is optimized but also the evaluation tolerance $\tau$; and it interleaves the selection of the training points and updating of the model with the sampling of the posterior distribution over $\Theta$, thus providing a solution to the IP task at hand at the same time as the surrogate model is trained. \medskip

A priori it is not clear if the seemingly more suitable assumptions of LR lead to similar or even better performance than the well-established GPR in the context of IP; similarly, the impact of retrieving the covariance structure of the discretization error on the performance of GPR can hardly be predicted.
Consequently, the final part of this work is dedicated to a number of numerical experiments, where the adaptive training of surrogates is tested on a number of different problems and compared to position adaptive and non adaptive training strategies.
Three different forward models with associated PDEs are considered: for the first two an analytical solution is available, rendering it possible to run a great number of training simulations and to evaluate the ground truth error; for the third one a more realistic model based on an elastomechanics problem is considered, requiring a FE discretization.\medskip

The contributions of this work can be summarized as follows: first, the LR technique will be formalized and generalized, explicitly considering its assumptions and providing a clear framework for its application in the context of IP; second, the formulation of a surrogate-based posterior distribution will be provided in a general form, explicitly stating the marginal likelihood to the LR case; third, an adaptive training strategy for stochastic surrogate models within a DoE framework will be proposed and new error metrics will be stated, extending preceding work to LR-based surrogates and including the estimate of the covariance structure of the discretization error for GPR-based surrogates; finally, a number of numerical experiments will be presented, comparing the performance of the training strategy on GPR and LR surrogates and the impact of the covariance structure of the discretization error on the performance of GPR-based surrogates.\medskip

The remainder of this work is organized as follows: in Section~\ref{sec:context} the context of the work is presented, with first an introduction to Bayesian Inverse Problems and their solution techniques, and then a discussion of Partial Differential Equations and Finite Element discretization.
In Section~\ref{sec:surrogates} the two surrogate techniques are introduced, presenting the established GPR and developing in generality LR, with a discussion of the assumptions made on the data and the model.
In Section~\ref{sec:likelihoods}, the IP is reformulated to account for the surrogate model, and the marginal likelihood is derived for both GPR and LR surrogates.
Section~\ref{sec:AL} presents the adaptive training strategy, first in a general form and then in the specific case of GPR and LR surrogates.
In Section~\ref{sec:exp} three different numerical experiments are presented, evaluating the performance of the proposed training strategy on GPR and LR-based surrogates; a final section regarding the impact of the covariance structure of the discretization error on the performance of GPR-based surrogates is also included.
Finally, in Section~\ref{sec:conclusions} conclusions are drawn and future work is discussed.

\subsection{Related work}\label{sec:literature}

Inverse problems and parameter identification tasks are widely studied in the literature, as they are ubiquitous across the most diverse fields of science and engineering~\cite{KaipioSomersalo2005}.
A variety of approaches to the solution of inverse problems exist, ranging from classical optimization techniques to Bayesian approaches~\cite{Tarantola2005}.
While point estimates of the parameters can be obtained by solving a minimization problem, often related to a least squares formulation with some regularization term~\cite{EngelHankeNeubauer1996}, for a Bayesian formulation sampling techniques are attractive due to their comprehensive representation of the uncertainty in the parameters posterior distribution~\cite{Sullivan2015}.
Markov Chain Monte Carlo (MCMC) methods are widely used for sampling from the posterior distribution, and a number of different techniques exist, ranging from the classical Metropolis-Hastings algorithm~\cite{MetropolisRosenbluthRosenbluthTellerTeller1953,Hastings1970} to successively developed techniques such as the Metropolis-adjusted Langevin algorithm~\cite{RobertsTweedie1996}, Ensemble Sampling~\cite{GoodmanWeare} and Differential Evolution MCMC~\cite{TerBraak}.\medskip

The advantages of sampling the posterior come at a price: in fact, sampling techniques usually require a significantly higher number of evaluations of the forward model than optimization techniques.
If the forward model is computationally expensive, this can render the direct adoption of sampling techniques impractical or even impossible.
To overcome this issue, a number of techniques have been proposed, often relying on the use of less expensive but less accurate models to substitute the forward model.
While approaches such as Delayed-acceptance MCMC~\cite{ChristenFox2005} and Multilevel MCMC~\cite{DodwellKetelsenScheichlTeckentrup2015} consider a hierarchy of models, utilizing the cheaper model to preselect promising candidates during the MCMC sampling, other approaches rely on the use of a surrogate model to replace the forward model entirely.
Asides from classical model reduction techniques~\cite{Schilders2008} such as Proper Orthogonal Decomposition~\cite{PateraRozza2007} and other Reduced Basis methods, most surrogate models are based on approximation techniques which do not relate directly to the underlying model.
There exists a vast literature on surrogate modeling in IPs, with a great variety of approaches including Polynomial chaos expansions~\cite{MarzoukNajmRahn2007}, sparse grid collocation~\cite{XiangZabaras2009}, Neural Networks~\cite{ZhouTartakovsky2021} and Gaussian process regression~\cite{Rasmussen2003}. \medskip

While its efficiency decreases with the dimensionality of the parameter space, Gaussian process regression (GPR)~\cite{RasmussenWilliams2006} is an attractive surrogate modeling technique for low to moderate-dimensional problems, as it does not require restrictions on the model form and quantifies the uncertainty in the prediction by definition.
The possibility of estimating the covariance structure of the forward model evaluation error via linear shrinkage estimators~\cite{LedoitWolf2004b} has not been considered in the context of GPR for IPs before; however, GPR has vastly been used in the context of IPs and the matter of the selection of training points has been widely studied.
Non-adaptive space-filling designs such as Latin Hypercube Sampling~\cite{McKayBeckmanConover1979} are available~\cite{Giunta}, but adaptive strategies can be considerably more efficient~\cite{Crombecq}.
If measurements are available before training, the surrogate can be trained to be more accurate in the region of interest while avoiding to spend computational resources in less relevant regions~\cite{SinsbeckNowak2017, WangBroccardo2020}.
Lipschitz regression (LR)~\cite{ZabinskySmithKristinsdottir2003,Calliess2017} has not been utilized in the context of IPs, but its features make it an interesting alternative to GPR.
In fact, LR does not require any prior distributional assumption on the data, and it is able to provide both a still stochastic prediction and hard bounds on the prediction error.\medskip

In this work, we consider the problem of adaptive training of both GPR and LR surrogate models.
Following preceding works~\cite{VillaniUngerWeiser2024,VillaniArconesUngerWeiser2025}, the sequential design of experiments approach is tailored towards a representation of the target posterior distribution, which is sampled during the training process. 
The training points are not selected at a fixed tolerance, but the forward model's evaluation tolerance is optimized as well. 
Such a tolerance optimization has been proposed for classical GPR~\cite{SemlerWeiser2023} and for gradient-enhanced GPR~\cite{SemlerWeiser2024}, delivering significant savings in computational costs without sacrificing the accuracy of the surrogate model.
The work of~\cite{Dinkel2024} aims at surrogating not the forward model but the likelihood function via GPR, and it interleaves the training of the surrogate with sampling of the posterior; the new points are selected randomly among the available samples. 
As such a training point selection strategy is similar to the one proposed in this work and can easily be adapted to the case of LR, we will use it as a benchmark for the performance of the proposed training strategy in the experiments of Section~\ref{sec:exp}.